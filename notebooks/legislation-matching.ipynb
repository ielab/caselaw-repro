{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates links between legislative provisions. I have not shared these pieces of legislation. However, in the features folder, one can find the generated link files. Legislation was parsed to a json format to be read by the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharType: \n",
    "    char = 0\n",
    "    num = 1\n",
    "    space = 2\n",
    "    other = 3\n",
    "\n",
    "def tokenize(text: str, lower: bool=True, ignore_punc: bool=True) -> List[str]:\n",
    "    curr = CharType.char\n",
    "    tokens = []\n",
    "    curr_word_len = 0\n",
    "    in_tag = False\n",
    "    \n",
    "    t_len = len(text)\n",
    "    \n",
    "    for i, t in enumerate(text): \n",
    "        prev = curr \n",
    "        if t.isdigit():\n",
    "            curr = CharType.num\n",
    "        elif t == ' ':\n",
    "            curr = CharType.space\n",
    "        elif t.isalpha():\n",
    "            curr = CharType.char\n",
    "        else:\n",
    "            curr = CharType.other\n",
    "            \n",
    "        change = False \n",
    "        \n",
    "        if prev != curr:\n",
    "            change = True \n",
    "        elif curr == CharType.other and i > 0 and text[i-1] != t:\n",
    "            change = True \n",
    "        \n",
    "\n",
    "        if change:\n",
    "            start = i-curr_word_len\n",
    "            if start < 0:\n",
    "                start = 0 \n",
    "\n",
    "            if curr_word_len != 0:\n",
    "                if not in_tag:\n",
    "                    if prev != CharType.other:\n",
    "                        tokens.append(text[start:i])\n",
    "                    elif not ignore_punc:\n",
    "                        tokens.append(text[start:i])\n",
    "\n",
    "                curr_word_len = 0\n",
    "\n",
    "        if i == t_len:\n",
    "            if curr != CharType.space:\n",
    "                if not_in_tag:\n",
    "                    if curr != CharType.other: \n",
    "                        tokens.append(text[i-curr_word_len:i+1])\n",
    "                    elif not ignore_punc:\n",
    "                        tokens.append(text[i-curr_word_len:i+1])\n",
    "\n",
    "                curr_word_len = 0\n",
    "\n",
    "        if curr != CharType.space and not in_tag:\n",
    "            curr_word_len += 1\n",
    "            \n",
    "        if curr == CharType.other:\n",
    "            if t == '<':\n",
    "                in_tag = True\n",
    "            elif t == '>':\n",
    "                in_tag = False\n",
    "          \n",
    "    if lower:\n",
    "        for i, t in enumerate(tokens):\n",
    "            tokens[i] = t.lower()\n",
    "\n",
    "    return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of repealed files\n",
    "\n",
    "repealed_leg = set()\n",
    "lookups = {}\n",
    "\n",
    "with open('/home/danlocke/legislation/comm_acts.json') as f:\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "        lookups[item['id'].lower()] = item\n",
    "        if item['repealed'] == 'Y':\n",
    "            repealed_leg.add(item['id'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/danlocke/legislation/comm_parsed'\n",
    "\n",
    "class Encoder: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self._lookup = {}\n",
    "        self._vocab= []\n",
    "        self._items = 0\n",
    "        \n",
    "    def add(self, tokens: List[str]):\n",
    "        for t in tokens:\n",
    "            if t not in self._lookup: \n",
    "                self._items += 1\n",
    "                self._lookup[t] = self._items\n",
    "                self._vocab.append(t)\n",
    "                \n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        ret = [0] * len(tokens)\n",
    "        for i, t in enumerate(tokens):\n",
    "            ret[i] = self._lookup.get(t, 0)\n",
    "            \n",
    "        return ret\n",
    "        \n",
    "encoder = Encoder()\n",
    "\n",
    "class DataHolder:\n",
    "    def __init__(self):\n",
    "        self._lookup = {}\n",
    "        self._lt_lookup ={}\n",
    "        self._lt = []\n",
    "        self._vals = []\n",
    "        self._titles = []\n",
    "        self._texts = []\n",
    "        self._long_title_vecs = None\n",
    "        self._lt_emb = None\n",
    "        self._title_vecs = None\n",
    "        self._text_vecs = None\n",
    "        \n",
    "    def add_long_title(self, act: str, lt: str):\n",
    "        self._lt_lookup[act] = len(self._lt)\n",
    "        self._lt.append(lt)\n",
    "        \n",
    "    def add(self, text: str, title: str, name: str):\n",
    "        if name in self._lookup:\n",
    "#             print(\"-\"*40)\n",
    "#             print(\"{0} already exists\".format(name))\n",
    "#             ind = self._lookup[name]\n",
    "#             print(self._titles[ind])\n",
    "#             print(self._texts[ind])\n",
    "#             print('*'*10)\n",
    "#             print(title)\n",
    "#             print(text)\n",
    "            return \n",
    "            \n",
    "        self._lookup[name] = len(self._vals)\n",
    "        self._vals.append(name)\n",
    "        self._titles.append(title)\n",
    "        self._texts.append(text)\n",
    "        \n",
    "    def add_vecs(self, title_vecs: csr_matrix, text_vecs: csr_matrix):\n",
    "        self._title_vecs = title_vecs\n",
    "        self._text_vecs = text_vecs\n",
    "        \n",
    "    def add_lt_vec(self, vec: csr_matrix):\n",
    "        self._long_title_vecs = vec\n",
    "        \n",
    "    def texts(self) -> List[str]:\n",
    "        return self._texts\n",
    "    \n",
    "    def titles(self) -> List[str]:\n",
    "        return self._titles\n",
    "    \n",
    "    def long_titles(self) -> List[str]:\n",
    "        return self._lt\n",
    "\n",
    "    def long_title_vecs(self) -> csr_matrix:\n",
    "        return self._long_title_vecs\n",
    "    \n",
    "    def title_vecs(self) -> csr_matrix:\n",
    "        return self._title_vecs\n",
    "    \n",
    "    def text_vecs(self) -> csr_matrix:\n",
    "        return self._text_vecs\n",
    "    \n",
    "    def print_row(self, row: int): \n",
    "        print(self._vals[row])\n",
    "        print(self._titles[row])\n",
    "        print(self._texts[row])\n",
    "        \n",
    "    def get_matching_rows_substr(self, sub: str) -> List[int]:\n",
    "        return [self._lookup[x] for x in self._vals if sub in x]\n",
    "    \n",
    "    def num_rows(self) -> int:\n",
    "        return self._titles.shape[0]\n",
    "    \n",
    "    def top_titles(self, k: int):\n",
    "        count_titles = {}\n",
    "        for title in self._titles:\n",
    "            if title not in count_titles:\n",
    "                count_titles[title] = 0\n",
    "            count_titles[title] += 1\n",
    "\n",
    "        n = 0\n",
    "        for k, v in sorted(count_titles.items(), key=lambda item: item[1], reverse=True):\n",
    "            print(k, v)\n",
    "            n += 1\n",
    "            if n > k:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2020c00130 s s\n",
      "c2013q00005xn01 s s\n",
      "f2010c00457 reg cl\n",
      "cl184 s s\n",
      "c2020c00120 s s\n",
      "c2010c00519 s s\n",
      "c2018c00342 s s\n",
      "f2017c00182 reg cl\n",
      "c2019c00103 s s\n",
      "c2020c00079 s s\n",
      "c2020c00084 s s\n",
      "c2019c00028 s s\n",
      "c2020c00137 s s\n"
     ]
    }
   ],
   "source": [
    "repealedDataHolder = DataHolder()\n",
    "currentDataHolder = DataHolder()\n",
    "\n",
    "default_prefix = 's'\n",
    "default_sch_prefix = 's'\n",
    "\n",
    "for root, d, files in os.walk(path):\n",
    "    for f_name in files:\n",
    "        if f_name.endswith('.json'):\n",
    "            act_name = f_name[:-5].lower()\n",
    "            if act_name[:2] == \"sl\":\n",
    "                continue\n",
    "            \n",
    "            prefix = lookups[act_name].get('prefix', default_prefix)\n",
    "            sch_prefix = lookups[act_name].get('schedule_prefix', default_sch_prefix)\n",
    "            \n",
    "            print(act_name, prefix, sch_prefix)\n",
    "                \n",
    "            with open(os.path.join(root, f_name)) as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                if 'body' not in data:\n",
    "                    continue\n",
    "                \n",
    "                enc_sch = False \n",
    "                enc_sch_name = ''\n",
    "                for i, item in enumerate(data['body']):\n",
    "                    section = item['tag'].replace('<b>', '').replace('</b>', '')\n",
    "                    \n",
    "\n",
    "                    if i == 0 or i == 1:\n",
    "                        joined = ' '.join(tokenize(item['tag']) + tokenize(item['type']))\n",
    "#                         print(joined)\n",
    "                        if 'an act' in joined:\n",
    "                            if act_name in repealed_leg:\n",
    "                                repealedDataHolder.add_long_title(act_name, joined)\n",
    "                            else: \n",
    "                                currentDataHolder.add_long_title(act_name, joined)\n",
    "                        continue\n",
    "                    \n",
    "                    if len(section) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    if section[0].isdigit():\n",
    "                        if enc_sch:\n",
    "                            section = '{0} {1} {2}'. format(enc_sch_name, sch_prefix, section)\n",
    "                        else:\n",
    "                            section = '{0} {1}'. format(prefix, section)\n",
    "                    elif \"Schedule\" in section:\n",
    "                        enc_sch = True\n",
    "                        enc_sch_name = section.replace(\"Schedule\", \"sch\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    title = ' '.join(tokenize(item['type']))\n",
    "#                     if title in exclude_some:\n",
    "#                         continue\n",
    "                   \n",
    "                    text = ' '.join(tokenize(item['text']))\n",
    "                    name = '{0}_{1}'.format(act_name, section)\n",
    "\n",
    "                    if act_name in repealed_leg: \n",
    "                        repealedDataHolder.add(text, title, name)\n",
    "                    else:\n",
    "                        currentDataHolder.add(text, title, name)\n",
    "#                     encoder.add(tokenize(item['text']))\n",
    "#                     encoder.add(tokenize(item['type']))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['the', 'of', 'to', 'in', 'for', 'that', 'and', 'on',\n",
       "                            'is', 'be', 'by', 'a', 'an', 'was', 'it', 'as',\n",
       "                            'this', 'which', 'with', 'have', 'at', 'been',\n",
       "                            'there', 'no', 'or', 'from', 'has', 'any', 'i',\n",
       "                            'would', ...])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['the', 'of', 'to', 'in', 'for', 'that', 'and','on', 'is',\n",
    "             'be', 'by', 'a', 'an', 'was', 'it', 'as', 'this', 'which', 'with', 'have', 'at', 'been', 'there',\n",
    "             'no', 'or', 'from', 'has', 'any', 'i', 'would', 'were', 'had', 'are', 'if', 'also','before', 'but', 'his', 'other',\n",
    "             'those', 'so', 'he', 'did', 'its', 'her', 'she', 'hers']\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, stop_words=stopwords)                                                                                                                                                                                                   \n",
    "vectorizer.fit(repealedDataHolder.titles() + currentDataHolder.titles() + repealedDataHolder.texts() + currentDataHolder.texts() + repealedDataHolder.long_titles() + currentDataHolder.long_titles())                                                                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path: str):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_vectors('/home/danlocke/fastText/filtered-100d.vec')\n",
    "\n",
    "\n",
    "def embed(tokens: List[str]) -> np.array:\n",
    "    e = [embeddings[x] for x in tokens if x in embeddings]\n",
    "    if len(e) == 0:\n",
    "        return np.zeros((100,))\n",
    "    return np.sum(e, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "repealedDataHolder.add_vecs(vectorizer.transform(repealedDataHolder.titles()), vectorizer.transform(repealedDataHolder.texts()))\n",
    "currentDataHolder.add_vecs(vectorizer.transform(currentDataHolder.titles()), vectorizer.transform(currentDataHolder.texts()))\n",
    "\n",
    "currentDataHolder.add_lt_vec(vectorizer.transform(currentDataHolder.long_titles()))\n",
    "repealedDataHolder.add_lt_vec(vectorizer.transform(repealedDataHolder.long_titles()))\n",
    "\n",
    "currentDataHolder._lt_emb = np.stack([embed(tokenize(x)) for x in currentDataHolder._lt], axis=0)\n",
    "repealedDataHolder._lt_emb = np.stack([embed(tokenize(x)) for x in repealedDataHolder._lt], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity objects of each act so we can do this as a matching criteria ..\n",
    "def get_objects(holder: DataHolder, _all: bool=False) -> List[int]:\n",
    "    if _all:\n",
    "        inds = [i for i in range(len(holder._titles))]\n",
    "    else:\n",
    "        inds = [i for i, x in enumerate(holder._titles) if x == \"objects\" or x == \"purpose\" or x == \"object\"]\n",
    "    acts = {holder._vals[x].split('_')[0]: x for x in inds}\n",
    "    return acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some typical legislative headings that we aren't really interested in \n",
    "exclude_some = {\n",
    "    'preliminary',\n",
    "    'short title',\n",
    "    'commencement',\n",
    "    'definitions',\n",
    "    'standard definitions',\n",
    "    'dictionary',\n",
    "    'application of act',\n",
    "    'object',\n",
    "    'interpretation',\n",
    "    'act binds all persons',\n",
    "    'key definitions',\n",
    "    'regulation making power',\n",
    "    'application of division',\n",
    "    'approved forms', \n",
    "    'approval of forms',\n",
    "    'interpretation',\n",
    "    'act binds the crown',\n",
    "}\n",
    "\n",
    "def get_links(a, b, exclude=exclude_some, match_lt: bool = False, match_obj: bool = False):\n",
    "    \n",
    "    same = a == b\n",
    "    a_objs = get_objects(a)\n",
    "    if same:\n",
    "        b_objs = a_objs\n",
    "    else:\n",
    "        b_objs = get_objects(b)\n",
    "\n",
    "    links = []\n",
    "    for row in range(a._title_vecs.shape[0]):\n",
    "        if a._texts[row] == '' or a._titles[row] == '' or a._titles[row] in exclude_some:\n",
    "            continue\n",
    "        \n",
    "        title_sims = cosine_similarity(a._title_vecs[row:row+1], b.title_vecs()).flatten()\n",
    "        text_sims = cosine_similarity(a._text_vecs[row:row+1], b.text_vecs()).flatten()\n",
    "            \n",
    "        inds = []\n",
    "        seen = set()\n",
    "        \n",
    "        sorted_text = text_sims.argsort()[:-10:-1]\n",
    "        for ind in sorted_text:\n",
    "            if same and ind == row:\n",
    "                continue\n",
    "            if text_sims[ind] < 0.95: \n",
    "                break \n",
    "                \n",
    "            inds.append(ind)\n",
    "            seen.add(ind)\n",
    "        \n",
    "        sorted_title = title_sims.argsort()[:-10:-1]\n",
    "        for ind in sorted_title:\n",
    "            if same and ind == row:\n",
    "                continue\n",
    "            if ind in seen:\n",
    "                continue\n",
    "            if title_sims[ind] < 0.9:\n",
    "                break\n",
    "            if b._texts[ind] == '':\n",
    "                continue\n",
    "            if 'definition' in a._titles[row] or 'application' in a._titles[row] or 'objects' in a._titles[row] or 'purpose' in a._titles[row]:\n",
    "                if text_sims[ind] < 0.9: \n",
    "                    continue\n",
    "            elif text_sims[ind] < 0.6: \n",
    "                    continue\n",
    "                \n",
    "            inds.append(ind)\n",
    "\n",
    "        if len(inds) == 0:\n",
    "            continue\n",
    "\n",
    "#         print('-'*100)\n",
    "#         currentDataHolder.print_row(row)\n",
    "        act_name = a._vals[row].split('_')[0]\n",
    "        lt_row = a._lt_lookup[act_name] if act_name in a._lt_lookup else None\n",
    "        object_row = a_objs[act_name] if act_name in a_objs else None\n",
    "        \n",
    "        # if lt_row is not None:\n",
    "        # print('\\nlong title: {0}'.format(currentDataHolder._lt[lt_row]))\n",
    "\n",
    "    #     if object_row is not None:\n",
    "    #         print(currentDataHolder._texts[object_row])\n",
    "\n",
    "        for ind in inds:\n",
    "#             print('*'*5)\n",
    "#             repealedDataHolder.print_row(ind)\n",
    "#             print(title_sims[ind], text_sims[ind])# len(currentDataHolder._text_vecs.getrow(row).nonzero()[0]), len(repealedDataHolder.text_vecs().getrow(ind).nonzero()[0]))\n",
    "            lt_sim = -1.0\n",
    "            lt_emb_sim = -1.0\n",
    "            obj_sim = -1.0\n",
    "            \n",
    "            match_act = b._vals[ind].split('_')[0]\n",
    "            if match_act in b._lt_lookup and lt_row is not None:\n",
    "                match_lt_row = b._lt_lookup[match_act]\n",
    "                lt_sim = cosine_similarity(a._long_title_vecs[lt_row:lt_row+1], b._long_title_vecs[match_lt_row:match_lt_row+1])[0][0]\n",
    "                lt_emb_sim = cosine_similarity(a._lt_emb[lt_row:lt_row+1], b._lt_emb[match_lt_row:match_lt_row+1])[0][0]\n",
    "                if match_lt and lt_emb_sim < 0.9:\n",
    "                    continue\n",
    "#                 print(cosine_similarity(currentDataHolder._long_title_vecs[lt_row:lt_row+1], repealedDataHolder._long_title_vecs[match_lt_row:match_lt_row+1])[0], \n",
    "#                      cosine_similarity(currentDataHolder._lt_emb[lt_row:lt_row+1], repealedDataHolder._lt_emb[match_lt_row:match_lt_row+1])[0])\n",
    "#                 print('matched long title: {0}'.format(repealedDataHolder._lt[match_lt_row]))\n",
    "            \n",
    "                if match_act in b_objs and object_row is not None:\n",
    "                    m_obj_ind = b_objs[match_act]\n",
    "                    obj_sim = cosine_similarity(a._text_vecs[object_row:object_row+1], b._text_vecs[m_obj_ind:m_obj_ind+1])[0][0]\n",
    "#                 print(pairwise_distances(currentDataHolder._text_vecs[object_row:object_row+1], repealedDataHolder._text_vecs[m_obj_ind:m_obj_ind+1], 'cosine'))\n",
    "#             print()\n",
    "            links.append({'from': a._vals[row], 'to': b._vals[ind], 'title_sim': float(title_sims[ind]), 'text_sim': float(text_sims[ind]), \n",
    "                          'lt_sim': float(lt_sim), 'lt_emb_sim': float(lt_emb_sim), 'obj_sim': float(obj_sim)})\n",
    "            \n",
    "    return links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_links = get_links(currentDataHolder, repealedDataHolder, match_lt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_curr_links = get_links(currentDataHolder, currentDataHolder, match_lt=False)\n",
    "# lt_rep_link = get_links(repealedDataHolder, repealedDataHolder, match_lt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_cnts = {}\n",
    "for link in lt_links: \n",
    "    _to = link['to'].split('_')[0]\n",
    "    _from = link['from'].split('_')[0]\n",
    "    if _from not in act_cnts:\n",
    "        act_cnts[_from] = {}\n",
    "\n",
    "    act_cnts[_from][_to] = act_cnts[_from].get(_to, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2020c00130 cl184 682 2213 6 0.008797653958944282\n",
      "c2020c00130 c2010c00519 682 1122 1 0.001466275659824047\n",
      "c2020c00120 c2010c00519 1025 1122 2 0.001951219512195122\n",
      "c2018c00342 c2010c00519 338 1122 4 0.011834319526627219\n",
      "f2017c00182 f2010c00457 45 43 13 0.3023255813953488\n",
      "c2019c00103 c2010c00519 546 1122 5 0.009157509157509158\n",
      "c2020c00079 c2010c00519 1489 1122 1074 0.9572192513368984\n",
      "c2020c00079 cl184 1489 2213 4 0.002686366689053056\n",
      "c2020c00084 c2010c00519 459 1122 52 0.11328976034858387\n",
      "c2020c00084 cl184 459 2213 9 0.0196078431372549\n",
      "c2019c00028 c2010c00519 83 1122 1 0.012048192771084338\n",
      "c2019c00028 cl184 83 2213 12 0.14457831325301204\n",
      "c2020c00137 cl184 3357 2213 1192 0.5386353366470854\n",
      "c2020c00137 c2010c00519 3357 1122 12 0.0106951871657754\n"
     ]
    }
   ],
   "source": [
    "def get_len(a, b, substr):\n",
    "    l = len(a.get_matching_rows_substr(substr))\n",
    "    if l == 0: \n",
    "        return len(b.get_matching_rows_substr(substr))\n",
    "    return l\n",
    "\n",
    "for k, v in act_cnts.items():\n",
    "    f_len = get_len(currentDataHolder, repealedDataHolder, k)\n",
    "        \n",
    "    for k2, v2 in v.items():\n",
    "        t_len = get_len(currentDataHolder, repealedDataHolder, k2)\n",
    "        small = f_len if f_len < t_len else t_len\n",
    "        print(k, k2, f_len, t_len, v2, float(v2) / float(small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 'f2017c00182_reg 4', 'to': 'f2010c00457_reg 3', 'title_sim': 1.0000000000000002, 'text_sim': 0.796114083287532, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 1', 'to': 'f2010c00457_sch cl 1', 'title_sim': 1.0, 'text_sim': 1.0000000000000002, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 2', 'to': 'f2010c00457_sch cl 2', 'title_sim': 1.0000000000000002, 'text_sim': 1.0, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 5', 'to': 'f2010c00457_sch cl 4', 'title_sim': 1.0, 'text_sim': 0.8384635246839994, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 8', 'to': 'f2010c00457_sch cl 6', 'title_sim': 1.0, 'text_sim': 0.6159874693323342, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 10', 'to': 'f2010c00457_sch cl 11', 'title_sim': 0.5759880567878783, 'text_sim': 0.9624264010642745, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 26', 'to': 'f2010c00457_sch cl 13', 'title_sim': 0.8746347254916162, 'text_sim': 0.9640100952573123, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 27', 'to': 'f2010c00457_sch cl 21', 'title_sim': 1.0000000000000002, 'text_sim': 0.8712155208248722, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 28', 'to': 'f2010c00457_sch cl 21', 'title_sim': 1.0000000000000002, 'text_sim': 0.8804145428356874, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 29', 'to': 'f2010c00457_sch cl 23', 'title_sim': 1.0, 'text_sim': 0.6963880370451909, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 33', 'to': 'f2010c00457_sch cl 15', 'title_sim': 1.0, 'text_sim': 0.8166273219042267, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 34', 'to': 'f2010c00457_sch cl 26', 'title_sim': 1.0000000000000004, 'text_sim': 0.6396748721936328, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 44', 'to': 'f2010c00457_sch cl 25', 'title_sim': 1.0, 'text_sim': 0.7700956872008782, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n"
     ]
    }
   ],
   "source": [
    "for i in lt_links:\n",
    "    if 'f2017' in i['from']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comm_links.json', 'w') as f:\n",
    "    json.dump(lt_links, f, indent=4)\n",
    "    \n",
    "with open('comm_curr_links.json', 'w') as f:\n",
    "    json.dump(lt_curr_links, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
