{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharType: \n",
    "    char = 0\n",
    "    num = 1\n",
    "    space = 2\n",
    "    other = 3\n",
    "\n",
    "def tokenize(text: str, lower: bool=True, ignore_punc: bool=True) -> List[str]:\n",
    "    curr = CharType.char\n",
    "    tokens = []\n",
    "    curr_word_len = 0\n",
    "    in_tag = False\n",
    "    \n",
    "    t_len = len(text)\n",
    "    \n",
    "    for i, t in enumerate(text): \n",
    "        prev = curr \n",
    "        if t.isdigit():\n",
    "            curr = CharType.num\n",
    "        elif t == ' ':\n",
    "            curr = CharType.space\n",
    "        elif t.isalpha():\n",
    "            curr = CharType.char\n",
    "        else:\n",
    "            curr = CharType.other\n",
    "            \n",
    "        change = False \n",
    "        \n",
    "        if prev != curr:\n",
    "            change = True \n",
    "        elif curr == CharType.other and i > 0 and text[i-1] != t:\n",
    "            change = True \n",
    "        \n",
    "\n",
    "        if change:\n",
    "            start = i-curr_word_len\n",
    "            if start < 0:\n",
    "                start = 0 \n",
    "\n",
    "            if curr_word_len != 0:\n",
    "                if not in_tag:\n",
    "                    if prev != CharType.other:\n",
    "                        tokens.append(text[start:i])\n",
    "                    elif not ignore_punc:\n",
    "                        tokens.append(text[start:i])\n",
    "\n",
    "                curr_word_len = 0\n",
    "\n",
    "        if i == t_len:\n",
    "            if curr != CharType.space:\n",
    "                if not_in_tag:\n",
    "                    if curr != CharType.other: \n",
    "                        tokens.append(text[i-curr_word_len:i+1])\n",
    "                    elif not ignore_punc:\n",
    "                        tokens.append(text[i-curr_word_len:i+1])\n",
    "\n",
    "                curr_word_len = 0\n",
    "\n",
    "        if curr != CharType.space and not in_tag:\n",
    "            curr_word_len += 1\n",
    "            \n",
    "        if curr == CharType.other:\n",
    "            if t == '<':\n",
    "                in_tag = True\n",
    "            elif t == '>':\n",
    "                in_tag = False\n",
    "          \n",
    "    if lower:\n",
    "        for i, t in enumerate(tokens):\n",
    "            tokens[i] = t.lower()\n",
    "\n",
    "    return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of repealed files\n",
    "\n",
    "repealed_leg = set()\n",
    "lookups = {}\n",
    "\n",
    "with open('/home/danlocke/legislation/comm_acts.json') as f:\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "        lookups[item['id'].lower()] = item\n",
    "        if item['repealed'] == 'Y':\n",
    "            repealed_leg.add(item['id'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/danlocke/legislation/comm_parsed'\n",
    "\n",
    "class Encoder: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self._lookup = {}\n",
    "        self._vocab= []\n",
    "        self._items = 0\n",
    "        \n",
    "    def add(self, tokens: List[str]):\n",
    "        for t in tokens:\n",
    "            if t not in self._lookup: \n",
    "                self._items += 1\n",
    "                self._lookup[t] = self._items\n",
    "                self._vocab.append(t)\n",
    "                \n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        ret = [0] * len(tokens)\n",
    "        for i, t in enumerate(tokens):\n",
    "            ret[i] = self._lookup.get(t, 0)\n",
    "            \n",
    "        return ret\n",
    "        \n",
    "encoder = Encoder()\n",
    "\n",
    "class DataHolder:\n",
    "    def __init__(self):\n",
    "        self._lookup = {}\n",
    "        self._lt_lookup ={}\n",
    "        self._lt = []\n",
    "        self._vals = []\n",
    "        self._titles = []\n",
    "        self._texts = []\n",
    "        self._long_title_vecs = None\n",
    "        self._lt_emb = None\n",
    "        self._title_vecs = None\n",
    "        self._text_vecs = None\n",
    "        \n",
    "    def add_long_title(self, act: str, lt: str):\n",
    "        self._lt_lookup[act] = len(self._lt)\n",
    "        self._lt.append(lt)\n",
    "        \n",
    "    def add(self, text: str, title: str, name: str):\n",
    "        if name in self._lookup:\n",
    "#             print(\"-\"*40)\n",
    "#             print(\"{0} already exists\".format(name))\n",
    "#             ind = self._lookup[name]\n",
    "#             print(self._titles[ind])\n",
    "#             print(self._texts[ind])\n",
    "#             print('*'*10)\n",
    "#             print(title)\n",
    "#             print(text)\n",
    "            return \n",
    "            \n",
    "        self._lookup[name] = len(self._vals)\n",
    "        self._vals.append(name)\n",
    "        self._titles.append(title)\n",
    "        self._texts.append(text)\n",
    "        \n",
    "    def add_vecs(self, title_vecs: csr_matrix, text_vecs: csr_matrix):\n",
    "        self._title_vecs = title_vecs\n",
    "        self._text_vecs = text_vecs\n",
    "        \n",
    "    def add_lt_vec(self, vec: csr_matrix):\n",
    "        self._long_title_vecs = vec\n",
    "        \n",
    "    def texts(self) -> List[str]:\n",
    "        return self._texts\n",
    "    \n",
    "    def titles(self) -> List[str]:\n",
    "        return self._titles\n",
    "    \n",
    "    def long_titles(self) -> List[str]:\n",
    "        return self._lt\n",
    "\n",
    "    def long_title_vecs(self) -> csr_matrix:\n",
    "        return self._long_title_vecs\n",
    "    \n",
    "    def title_vecs(self) -> csr_matrix:\n",
    "        return self._title_vecs\n",
    "    \n",
    "    def text_vecs(self) -> csr_matrix:\n",
    "        return self._text_vecs\n",
    "    \n",
    "    def print_row(self, row: int): \n",
    "        print(self._vals[row])\n",
    "        print(self._titles[row])\n",
    "        print(self._texts[row])\n",
    "        \n",
    "    def get_matching_rows_substr(self, sub: str) -> List[int]:\n",
    "        return [self._lookup[x] for x in self._vals if sub in x]\n",
    "    \n",
    "    def num_rows(self) -> int:\n",
    "        return self._titles.shape[0]\n",
    "    \n",
    "    def top_titles(self, k: int):\n",
    "        count_titles = {}\n",
    "        for title in self._titles:\n",
    "            if title not in count_titles:\n",
    "                count_titles[title] = 0\n",
    "            count_titles[title] += 1\n",
    "\n",
    "        n = 0\n",
    "        for k, v in sorted(count_titles.items(), key=lambda item: item[1], reverse=True):\n",
    "            print(k, v)\n",
    "            n += 1\n",
    "            if n > k:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2020c00130 s s\n",
      "c2013q00005xn01 s s\n",
      "f2010c00457 reg cl\n",
      "cl184 s s\n",
      "c2020c00120 s s\n",
      "c2010c00519 s s\n",
      "c2018c00342 s s\n",
      "f2017c00182 reg cl\n",
      "c2019c00103 s s\n",
      "c2020c00079 s s\n",
      "c2020c00084 s s\n",
      "c2019c00028 s s\n",
      "c2020c00137 s s\n"
     ]
    }
   ],
   "source": [
    "repealedDataHolder = DataHolder()\n",
    "currentDataHolder = DataHolder()\n",
    "\n",
    "default_prefix = 's'\n",
    "default_sch_prefix = 's'\n",
    "\n",
    "for root, d, files in os.walk(path):\n",
    "    for f_name in files:\n",
    "        if f_name.endswith('.json'):\n",
    "            act_name = f_name[:-5].lower()\n",
    "            if act_name[:2] == \"sl\":\n",
    "                continue\n",
    "            \n",
    "            prefix = lookups[act_name].get('prefix', default_prefix)\n",
    "            sch_prefix = lookups[act_name].get('schedule_prefix', default_sch_prefix)\n",
    "            \n",
    "            print(act_name, prefix, sch_prefix)\n",
    "                \n",
    "            with open(os.path.join(root, f_name)) as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                if 'body' not in data:\n",
    "                    continue\n",
    "                \n",
    "                enc_sch = False \n",
    "                enc_sch_name = ''\n",
    "                for i, item in enumerate(data['body']):\n",
    "                    section = item['tag'].replace('<b>', '').replace('</b>', '')\n",
    "                    \n",
    "\n",
    "                    if i == 0 or i == 1:\n",
    "                        joined = ' '.join(tokenize(item['tag']) + tokenize(item['type']))\n",
    "#                         print(joined)\n",
    "                        if 'an act' in joined:\n",
    "                            if act_name in repealed_leg:\n",
    "                                repealedDataHolder.add_long_title(act_name, joined)\n",
    "                            else: \n",
    "                                currentDataHolder.add_long_title(act_name, joined)\n",
    "                        continue\n",
    "                    \n",
    "                    if len(section) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    if section[0].isdigit():\n",
    "                        if enc_sch:\n",
    "                            section = '{0} {1} {2}'. format(enc_sch_name, sch_prefix, section)\n",
    "                        else:\n",
    "                            section = '{0} {1}'. format(prefix, section)\n",
    "                    elif \"Schedule\" in section:\n",
    "                        enc_sch = True\n",
    "                        enc_sch_name = section.replace(\"Schedule\", \"sch\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    title = ' '.join(tokenize(item['type']))\n",
    "#                     if title in exclude_some:\n",
    "#                         continue\n",
    "                   \n",
    "                    text = ' '.join(tokenize(item['text']))\n",
    "                    name = '{0}_{1}'.format(act_name, section)\n",
    "\n",
    "                    if act_name in repealed_leg: \n",
    "                        repealedDataHolder.add(text, title, name)\n",
    "                    else:\n",
    "                        currentDataHolder.add(text, title, name)\n",
    "#                     encoder.add(tokenize(item['text']))\n",
    "#                     encoder.add(tokenize(item['type']))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['the', 'of', 'to', 'in', 'for', 'that', 'and', 'on',\n",
       "                            'is', 'be', 'by', 'a', 'an', 'was', 'it', 'as',\n",
       "                            'this', 'which', 'with', 'have', 'at', 'been',\n",
       "                            'there', 'no', 'or', 'from', 'has', 'any', 'i',\n",
       "                            'would', ...])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['the', 'of', 'to', 'in', 'for', 'that', 'and','on', 'is',\n",
    "             'be', 'by', 'a', 'an', 'was', 'it', 'as', 'this', 'which', 'with', 'have', 'at', 'been', 'there',\n",
    "             'no', 'or', 'from', 'has', 'any', 'i', 'would', 'were', 'had', 'are', 'if', 'also','before', 'but', 'his', 'other',\n",
    "             'those', 'so', 'he', 'did', 'its', 'her', 'she', 'hers']\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, stop_words=stopwords)                                                                                                                                                                                                   \n",
    "vectorizer.fit(repealedDataHolder.titles() + currentDataHolder.titles() + repealedDataHolder.texts() + currentDataHolder.texts() + repealedDataHolder.long_titles() + currentDataHolder.long_titles())                                                                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path: str):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_vectors('/home/danlocke/fastText/filtered-100d.vec')\n",
    "\n",
    "\n",
    "def embed(tokens: List[str]) -> np.array:\n",
    "    e = [embeddings[x] for x in tokens if x in embeddings]\n",
    "    if len(e) == 0:\n",
    "        return np.zeros((100,))\n",
    "    return np.sum(e, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "repealedDataHolder.add_vecs(vectorizer.transform(repealedDataHolder.titles()), vectorizer.transform(repealedDataHolder.texts()))\n",
    "currentDataHolder.add_vecs(vectorizer.transform(currentDataHolder.titles()), vectorizer.transform(currentDataHolder.texts()))\n",
    "\n",
    "currentDataHolder.add_lt_vec(vectorizer.transform(currentDataHolder.long_titles()))\n",
    "repealedDataHolder.add_lt_vec(vectorizer.transform(repealedDataHolder.long_titles()))\n",
    "\n",
    "currentDataHolder._lt_emb = np.stack([embed(tokenize(x)) for x in currentDataHolder._lt], axis=0)\n",
    "repealedDataHolder._lt_emb = np.stack([embed(tokenize(x)) for x in repealedDataHolder._lt], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity objects of each act so we can do this as a matching criteria ..\n",
    "def get_objects(holder: DataHolder, _all: bool=False) -> List[int]:\n",
    "    if _all:\n",
    "        inds = [i for i in range(len(holder._titles))]\n",
    "    else:\n",
    "        inds = [i for i, x in enumerate(holder._titles) if x == \"objects\" or x == \"purpose\" or x == \"object\"]\n",
    "    acts = {holder._vals[x].split('_')[0]: x for x in inds}\n",
    "    return acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some typical legislative headings that we aren't really interested in \n",
    "exclude_some = {\n",
    "    'preliminary',\n",
    "    'short title',\n",
    "    'commencement',\n",
    "    'definitions',\n",
    "    'standard definitions',\n",
    "    'dictionary',\n",
    "    'application of act',\n",
    "    'object',\n",
    "    'interpretation',\n",
    "    'act binds all persons',\n",
    "    'key definitions',\n",
    "    'regulation making power',\n",
    "    'application of division',\n",
    "    'approved forms', \n",
    "    'approval of forms',\n",
    "    'interpretation',\n",
    "    'act binds the crown',\n",
    "}\n",
    "\n",
    "def get_links(a, b, exclude=exclude_some, match_lt: bool = False, match_obj: bool = False):\n",
    "    \n",
    "    same = a == b\n",
    "    a_objs = get_objects(a)\n",
    "    if same:\n",
    "        b_objs = a_objs\n",
    "    else:\n",
    "        b_objs = get_objects(b)\n",
    "\n",
    "    links = []\n",
    "    for row in range(a._title_vecs.shape[0]):\n",
    "        if a._texts[row] == '' or a._titles[row] == '' or a._titles[row] in exclude_some:\n",
    "            continue\n",
    "        \n",
    "        title_sims = cosine_similarity(a._title_vecs[row:row+1], b.title_vecs()).flatten()\n",
    "        text_sims = cosine_similarity(a._text_vecs[row:row+1], b.text_vecs()).flatten()\n",
    "            \n",
    "        inds = []\n",
    "        seen = set()\n",
    "        \n",
    "        sorted_text = text_sims.argsort()[:-10:-1]\n",
    "        for ind in sorted_text:\n",
    "            if same and ind == row:\n",
    "                continue\n",
    "            if text_sims[ind] < 0.95: \n",
    "                break \n",
    "                \n",
    "            inds.append(ind)\n",
    "            seen.add(ind)\n",
    "        \n",
    "        sorted_title = title_sims.argsort()[:-10:-1]\n",
    "        for ind in sorted_title:\n",
    "            if same and ind == row:\n",
    "                continue\n",
    "            if ind in seen:\n",
    "                continue\n",
    "            if title_sims[ind] < 0.9:\n",
    "                break\n",
    "            if b._texts[ind] == '':\n",
    "                continue\n",
    "            if 'definition' in a._titles[row] or 'application' in a._titles[row] or 'objects' in a._titles[row] or 'purpose' in a._titles[row]:\n",
    "                if text_sims[ind] < 0.9: \n",
    "                    continue\n",
    "            elif text_sims[ind] < 0.6: \n",
    "                    continue\n",
    "                \n",
    "            inds.append(ind)\n",
    "\n",
    "        if len(inds) == 0:\n",
    "            continue\n",
    "\n",
    "#         print('-'*100)\n",
    "#         currentDataHolder.print_row(row)\n",
    "        act_name = a._vals[row].split('_')[0]\n",
    "        lt_row = a._lt_lookup[act_name] if act_name in a._lt_lookup else None\n",
    "        object_row = a_objs[act_name] if act_name in a_objs else None\n",
    "        \n",
    "        # if lt_row is not None:\n",
    "        # print('\\nlong title: {0}'.format(currentDataHolder._lt[lt_row]))\n",
    "\n",
    "    #     if object_row is not None:\n",
    "    #         print(currentDataHolder._texts[object_row])\n",
    "\n",
    "        for ind in inds:\n",
    "#             print('*'*5)\n",
    "#             repealedDataHolder.print_row(ind)\n",
    "#             print(title_sims[ind], text_sims[ind])# len(currentDataHolder._text_vecs.getrow(row).nonzero()[0]), len(repealedDataHolder.text_vecs().getrow(ind).nonzero()[0]))\n",
    "            lt_sim = -1.0\n",
    "            lt_emb_sim = -1.0\n",
    "            obj_sim = -1.0\n",
    "            \n",
    "            match_act = b._vals[ind].split('_')[0]\n",
    "            if match_act in b._lt_lookup and lt_row is not None:\n",
    "                match_lt_row = b._lt_lookup[match_act]\n",
    "                lt_sim = cosine_similarity(a._long_title_vecs[lt_row:lt_row+1], b._long_title_vecs[match_lt_row:match_lt_row+1])[0][0]\n",
    "                lt_emb_sim = cosine_similarity(a._lt_emb[lt_row:lt_row+1], b._lt_emb[match_lt_row:match_lt_row+1])[0][0]\n",
    "                if match_lt and lt_emb_sim < 0.9:\n",
    "                    continue\n",
    "#                 print(cosine_similarity(currentDataHolder._long_title_vecs[lt_row:lt_row+1], repealedDataHolder._long_title_vecs[match_lt_row:match_lt_row+1])[0], \n",
    "#                      cosine_similarity(currentDataHolder._lt_emb[lt_row:lt_row+1], repealedDataHolder._lt_emb[match_lt_row:match_lt_row+1])[0])\n",
    "#                 print('matched long title: {0}'.format(repealedDataHolder._lt[match_lt_row]))\n",
    "            \n",
    "                if match_act in b_objs and object_row is not None:\n",
    "                    m_obj_ind = b_objs[match_act]\n",
    "                    obj_sim = cosine_similarity(a._text_vecs[object_row:object_row+1], b._text_vecs[m_obj_ind:m_obj_ind+1])[0][0]\n",
    "#                 print(pairwise_distances(currentDataHolder._text_vecs[object_row:object_row+1], repealedDataHolder._text_vecs[m_obj_ind:m_obj_ind+1], 'cosine'))\n",
    "#             print()\n",
    "            links.append({'from': a._vals[row], 'to': b._vals[ind], 'title_sim': float(title_sims[ind]), 'text_sim': float(text_sims[ind]), \n",
    "                          'lt_sim': float(lt_sim), 'lt_emb_sim': float(lt_emb_sim), 'obj_sim': float(obj_sim)})\n",
    "            \n",
    "    return links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_links = get_links(currentDataHolder, repealedDataHolder, match_lt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_curr_links = get_links(currentDataHolder, currentDataHolder, match_lt=False)\n",
    "# lt_rep_link = get_links(repealedDataHolder, repealedDataHolder, match_lt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_cnts = {}\n",
    "for link in lt_links: \n",
    "    _to = link['to'].split('_')[0]\n",
    "    _from = link['from'].split('_')[0]\n",
    "    if _from not in act_cnts:\n",
    "        act_cnts[_from] = {}\n",
    "\n",
    "    act_cnts[_from][_to] = act_cnts[_from].get(_to, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2020c00130 cl184 682 2213 6 0.008797653958944282\n",
      "c2020c00130 c2010c00519 682 1122 1 0.001466275659824047\n",
      "c2020c00120 c2010c00519 1025 1122 2 0.001951219512195122\n",
      "c2018c00342 c2010c00519 338 1122 4 0.011834319526627219\n",
      "f2017c00182 f2010c00457 45 43 13 0.3023255813953488\n",
      "c2019c00103 c2010c00519 546 1122 5 0.009157509157509158\n",
      "c2020c00079 c2010c00519 1489 1122 1074 0.9572192513368984\n",
      "c2020c00079 cl184 1489 2213 4 0.002686366689053056\n",
      "c2020c00084 c2010c00519 459 1122 52 0.11328976034858387\n",
      "c2020c00084 cl184 459 2213 9 0.0196078431372549\n",
      "c2019c00028 c2010c00519 83 1122 1 0.012048192771084338\n",
      "c2019c00028 cl184 83 2213 12 0.14457831325301204\n",
      "c2020c00137 cl184 3357 2213 1192 0.5386353366470854\n",
      "c2020c00137 c2010c00519 3357 1122 12 0.0106951871657754\n"
     ]
    }
   ],
   "source": [
    "def get_len(a, b, substr):\n",
    "    l = len(a.get_matching_rows_substr(substr))\n",
    "    if l == 0: \n",
    "        return len(b.get_matching_rows_substr(substr))\n",
    "    return l\n",
    "\n",
    "for k, v in act_cnts.items():\n",
    "    f_len = get_len(currentDataHolder, repealedDataHolder, k)\n",
    "        \n",
    "    for k2, v2 in v.items():\n",
    "        t_len = get_len(currentDataHolder, repealedDataHolder, k2)\n",
    "        small = f_len if f_len < t_len else t_len\n",
    "        print(k, k2, f_len, t_len, v2, float(v2) / float(small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 'f2017c00182_reg 4', 'to': 'f2010c00457_reg 3', 'title_sim': 1.0000000000000002, 'text_sim': 0.796114083287532, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 1', 'to': 'f2010c00457_sch cl 1', 'title_sim': 1.0, 'text_sim': 1.0000000000000002, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 2', 'to': 'f2010c00457_sch cl 2', 'title_sim': 1.0000000000000002, 'text_sim': 1.0, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 5', 'to': 'f2010c00457_sch cl 4', 'title_sim': 1.0, 'text_sim': 0.8384635246839994, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 8', 'to': 'f2010c00457_sch cl 6', 'title_sim': 1.0, 'text_sim': 0.6159874693323342, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 10', 'to': 'f2010c00457_sch cl 11', 'title_sim': 0.5759880567878783, 'text_sim': 0.9624264010642745, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 26', 'to': 'f2010c00457_sch cl 13', 'title_sim': 0.8746347254916162, 'text_sim': 0.9640100952573123, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 27', 'to': 'f2010c00457_sch cl 21', 'title_sim': 1.0000000000000002, 'text_sim': 0.8712155208248722, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 28', 'to': 'f2010c00457_sch cl 21', 'title_sim': 1.0000000000000002, 'text_sim': 0.8804145428356874, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 29', 'to': 'f2010c00457_sch cl 23', 'title_sim': 1.0, 'text_sim': 0.6963880370451909, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 33', 'to': 'f2010c00457_sch cl 15', 'title_sim': 1.0, 'text_sim': 0.8166273219042267, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 34', 'to': 'f2010c00457_sch cl 26', 'title_sim': 1.0000000000000004, 'text_sim': 0.6396748721936328, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n",
      "{'from': 'f2017c00182_sch 1 cl 44', 'to': 'f2010c00457_sch cl 25', 'title_sim': 1.0, 'text_sim': 0.7700956872008782, 'lt_sim': -1.0, 'lt_emb_sim': -1.0, 'obj_sim': -1.0}\n"
     ]
    }
   ],
   "source": [
    "for i in lt_links:\n",
    "    if 'f2017' in i['from']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comm_links.json', 'w') as f:\n",
    "    json.dump(lt_links, f, indent=4)\n",
    "    \n",
    "with open('comm_curr_links.json', 'w') as f:\n",
    "    json.dump(lt_curr_links, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names = ['lt_links', 'lt_curr_links', 'lt_rep_links']\n",
    "# datas = [lt_links, lt_curr_links, lt_rep_link]\n",
    "# for i, f_name in enumerate(file_names):\n",
    "#     with open(f_name+'.json', 'w') as f:\n",
    "#         json.dump(datas[i], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/danlocke/go/src/github.com/dan-locke/phd-data/case-topics.json', 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the effect of reinstating a company that was in liquidation as regards money that may be recovered?\n",
      "Is the variation of the date for settlement required to be in writing?\n",
      "Maintenance and champerty and the requisite degree of control\n",
      "agency fees and effective cause of sale of a boat\n",
      "Should damages be reduced according to a sum that represents betterment\n",
      "Consideration of a clause that provides that the contract is subject to the buyer being satisfied, in its absolute discretion, with the due diligence by a date\n",
      "Maintenance and champerty\n",
      "proportionality as a basis for striking out a defamation claim\n",
      "In trade or commerce\n",
      "Whether membership of an organisation is in trade or commerce\n",
      "Organisations owing members a duty of care\n",
      "That proprietary relief is only available after rescission \n",
      "Exemplary damages and retaining the benefit of any brokerage being a basis for their assessment\n",
      "Google search results and defamation\n",
      "Postponing the giving of particulars of a claim until after discovery\n",
      "Bare trustee duties\n",
      "Valuation of future losses with limited evidence\n",
      "Business profitability as a basis for assessing damages\n",
      "Administrative decision making and the Briginshaw standard of proof\n",
      "Common intention constructive trusts\n",
      "Restitutionary relief and entitlement to a benefit\n",
      "Interlocutory applications and res judicatums\n",
      "whether future profit on resale are to be brought to bear on a question of loss of opportunity\n",
      "Whether limitation periods are substantive law for the purpose of a conflict of laws\n",
      "Whether in a common intention constructive trust the intention must be a continuing one\n",
      "Stay of a QCAT order revoking a license\n",
      "The appropriate test for severance of a joint tenancy\n",
      "Severance of a joint tenancy by conduct by means of discussions and agreements to sell property held as joint tenants\n",
      "Damages for defamation compensate for, amongst other things, emotional distress\n",
      "omission of equal in dispositive clause providing for division of property in a will\n",
      "Liability under contract for breach where performance is subcontracted\n",
      "equitable jurisidiction to set aside a perfected judgment in criminal case on the basis of fraud\n",
      "Admission of fresh or new evidence\n",
      "Obligation of prosecution to investigate as distinct from the obligation to disclose\n",
      "Whether incompetence of trial counsel is a basis for a criminal appeal\n",
      "Relevant considerations for an application for summary judgment in the Federal Court\n",
      "Recovery of costs in estate litigation matters\n",
      "Right of first refusal and restraints on alienation\n",
      "Causation and evidentiary standards\n",
      "Expectation losses and misleading or deceptive conduct\n",
      "Redaction of documents for disclosure\n",
      "Hypothetical nature of declarations\n",
      "Nunc pro tunc\n",
      "motor vehicle accidents involving caravans\n",
      "contributory negligence in motor vehicle accident cases\n",
      "amending pleadings after trial date set down\n",
      "set off and suspension clauses\n",
      "Non party costs against a party's solicitor\n",
      "surrender of an easement\n",
      "Whether a settlement agreement between a company and some of its directors, binds all directors\n",
      "the need to show mismanagement to get damages when pleading breach of fiduciary duties and the remedies available\n",
      "Causation and reliance\n",
      "judicial review and unreasonableness / intelligible justification\n",
      "Sale of land and entitlement of vendor to specific performance\n",
      "Issues with duty to other party where procuring report\n",
      "Malicious prosecution and witness immunity from suit\n",
      "Ex debito justitiae\n",
      "security for costs and delay\n",
      "success in seeking security for costs where there has been a delay\n",
      "whether administration validly entered into or entered into under improper purpose\n",
      "Construction of broad definition clauses in professional indemnity insurance policies\n",
      "meaning of dispute\n",
      "meaning justiciable controversy\n",
      "defintion of a mortgage\n",
      "Discretion as to costs\n",
      "where a document specifies the costs that a party may claim from another, is the Court bound to follow the document, or can it fix costs in its own discretion? \n",
      "where a party may be subject to an indemnity costs order for their conduct, how does this intersect with a contractual entitlement to indemnity costs\n",
      "Doctrine of ultimate effect\n",
      "Meaning of transaction in the doctrine of ultimate effect\n",
      "meaning of debt\n",
      "Concurrent expert evidence\n",
      "Contract and intention to create legal relations\n",
      "Whether a presumption exists against the creation of legal relations in family relationships\n",
      "liquidator impartiality\n",
      "Litigation guardians\n",
      "Removal of a litigation guardian\n",
      "defamation pleading of publication and context\n",
      "defamation republication\n",
      "reasonableness of reliance for misleading or deceptive conduct\n",
      "Equitable set off\n",
      "stay of actions during bankruptcy\n",
      "staying proceedings commenced by a person who subsequently becomes bankrupt\n",
      "what amounts to a genuine commercial interest such that the assignment of a right to litigate is not a bare right to litigate\n",
      "unclean hands and immediate and necessary connection\n",
      "To what extent will a partyâ€™s participation in conduct of the other party that is the basis of the defence of unclean hands disentitle that party from relying upon the defence\n",
      "Where a party in a without prejudice situation puts forward a situation that is different to the basis of their claim or defence can that contradictory situation be put to the witness\n",
      "Where an insured settles a claim against it, what does it have to do to establish the settlement was reasonable in a third party claim against its insurer that denied the indemnity\n",
      "Whether a subsequent offer letter not marked without prejudice is privileged as being without prejudice\n",
      "can a party appeal a decision where they filed an appearance but took no active role in the proceedings\n",
      "where a joint venture company has a receiver appointed pursuant to a security and other assets of the company would satisfy the security, can another funder seek access to an asset\n",
      "injuries to children in foster care\n",
      "is privilege waived by a client that seeks an order that their former laywer should pay costs?\n",
      "the effect of death of a party on a cause of action and costs orders\n",
      "motor vehicle accidents involving motorcycles\n",
      "Sale of land specific performance and adequacy of damages\n"
     ]
    }
   ],
   "source": [
    "for topic in data['topics']:\n",
    "    print(topic['topic'])\n",
    "#     tokens = tokenize(topic['topic'])\n",
    "#     vec = vectorizer.transform([' '.join(tokens)])\n",
    "#     sims = cosine_similarity(vec[0:1], currentDataHolder._text_vecs)[0]\n",
    "#     sim_inds = sims.argsort()[:-10:-1]\n",
    "#     for ind in sim_inds:\n",
    "# #         if sims[ind] < 0.5: \n",
    "# #             continue\n",
    "#         currentDataHolder.print_row(ind)\n",
    "#         print(sims[ind])\n",
    "#     print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenize('Is the variation of the date for settlement required to be in writing?')\n",
    "# vec = vectorizer.transform([' '.join(tokens)])\n",
    "# rows = currentDataHolder.get_matching_rows_substr('act-1974-076')\n",
    "# sims = cosine_similarity(vec, currentDataHolder._text_vecs[rows[0]:rows[-1]])[0]\n",
    "# sim_inds = sims.argsort()[:-10:-1]\n",
    "# for ind in sim_inds:\n",
    "#     currentDataHolder.print_row(rows[0]+ind)\n",
    "#     print(sims[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
