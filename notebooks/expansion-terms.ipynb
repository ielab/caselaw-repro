{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for getting top-k embedding expansion terms for Kuzi centroid method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path: str) -> Tuple[Dict[str, int], List[str], np.ndarray]:\n",
    "    vocab = []\n",
    "    lookup = {}\n",
    "    vecs = []\n",
    "    ind = 0\n",
    "    with open(path,'r') as f:\n",
    "        f.readline()\n",
    "\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            vocab.append(split_line[0])\n",
    "            lookup[split_line[0]] = ind\n",
    "            ind += 1\n",
    "            vecs.append(np.array(split_line[1:], dtype=np.float64))\n",
    "        \n",
    "    return lookup, vocab, np.asarray(vecs, dtype=np.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_path = '../embeddings/filtered-100d.vec'\n",
    "lookup, vocab, embs = load_embeddings(emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"damages defamation compensate amongst other things emotional distress\".split()\n",
    "# query = \"maintenance champerty requisite degree control\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(term: str, lookup: Dict[str, int], embs: np.ndarray) -> np.ndarray:\n",
    "    if term not in lookup:\n",
    "        raise Exception(term)\n",
    "\n",
    "    return embs[lookup[term]]\n",
    "\n",
    "centroid = np.sum(np.asarray([get_embedding(q, lookup, embs) for q in query]), axis=0)\n",
    "centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top(vec: np.ndarray, embs: np.ndarray, vocab: List[str], k: int = 20, sim: int = None) -> None:\n",
    "    similarities = cosine_similarity(vec.reshape(1, -1), embs)\n",
    "    if sim != None:\n",
    "        print(similarities[0][sim])\n",
    "    inds = np.argsort(similarities)\n",
    "    for ind in inds[0][-k:]:\n",
    "        print(ind, vocab[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top(centroid, embs, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top(get_embedding('damages', lookup, embs), embs, vocab, sim=415)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
