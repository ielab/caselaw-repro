{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for BERT sentence sim nearest neighbour, MsMarco trained sentence, passage and window scores, as well as interpolation of these scores with DP, and upperbound calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "import copy \n",
    "import numpy as np\n",
    "\n",
    "nb_dir = os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from plotlib.loaders import *\n",
    "from plotlib.plotters import *\n",
    "\n",
    "from phdconf import config \n",
    "from phdconf.config import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(config.AUS_TOPIC_PATH)\n",
    "broad, specific = load_query_types(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = ['bert-para', 'bert-cleaned', 'fasttext']\n",
    "qrel_paths = [config.AUS_QREL_PATH] * len(index_names)\n",
    "rel_levels = [config.AUS_REL_LEVEL] * len(index_names)\n",
    "display_names = ['bert', 'fasttext']\n",
    "\n",
    "file_names = 'case-topics-{0}-vector-search.run'\n",
    "\n",
    "# go_path = os.path.join(os.environ[\"HOME\"], 'go/src/github.com/dan-locke')\n",
    "path = os.path.join(BASE_DIR, 'bert-rerank')\n",
    "# curr_path = os.path.join(os.environ[\"HOME\"], 'go/src/github.com/dan-locke/phd/python-scripts')\n",
    "# dfs = load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, curr_path, runs)\n",
    "\n",
    "metrics = copy.deepcopy(config.METRIC_NAMES)\n",
    "metrics['unjudged@20'] = 'Unjudged@20'\n",
    "\n",
    "\n",
    "dfs = load_1d_dfs(index_names, qrel_paths, BASE_DIR, file_names, rel_levels, 1, 1, 1)\n",
    "# broad_dfs = load_1d_dfs(index_names, qrel_paths, BASE_DIR, file_names, rel_levels, 1, 1, 1, filtered=broad)\n",
    "# specific_dfs = load_1d_dfs(index_names, qrel_paths, BASE_DIR, file_names, rel_levels, 1, 1, 1, filtered=specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = load_1d_dfs(['filtered-phrasestop'], qrel_paths, os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), 'case-topics-{0}-unigram_dir_mu_{1:.2f}.run', rel_levels, 1050, 1050, 1)[0][0]\n",
    "base_qry = load_1d_dfs(['filtered-phrasestop'], qrel_paths, os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), 'case-topics-{0}-unigram_dir_mu_{1:.2f}.run', rel_levels, 1050, 1050, 1, per_query=True)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT sentence similarity nearest neighbour Annoy lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_files = []\n",
    "topic = 'case-topics'\n",
    "run_names = ['filtered-phrasestop-unigram_dir_mu_1050.00.run', 'bert-para-vector-search.run', 'bert-cleaned-vector-search.run', 'fasttext-vector-search.run']\n",
    "names = ['$R$', 'BERT-para', 'BERT-cleaned', 'fasttext']\n",
    "dirs = [os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), BASE_DIR, BASE_DIR, BASE_DIR]\n",
    "for i, r in enumerate(run_names):\n",
    "    ev_files.append((names[i], os.path.join(dirs[i], topic+'-'+r), None))\n",
    "\n",
    "b_df = compute_stat_sig(ev_files, config.AUS_QREL_PATH, ['$R$', 'BERT-para', 'BERT-cleaned'], metrics, rel_level='1')\n",
    "# write_table('tables/ausnl-neighbour', bold_max(b_df[metrics.keys()]).drop('unjudged@20',axis='columns').rename(config.METRIC_NAMES, axis='columns').to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT passage, sentence and window scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_dfs = load_1d_dfs(['case-topics'], qrel_paths, path, '{0}-tinybert-nrm-sum-{1}.run', rel_levels, 1, 10, 1)\n",
    "all_dfs = load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, path, ['case-topics-tinybert-nrm-'+str(y)+'-'+x+'.run' for x in  ['gauss', 'inv', 'exp', 'wind'] for y in  [2, 5, 10, 20, 50, 75, 100, 150, 200]])\n",
    "sum_psg_dfs = load_1d_dfs(['case-topics'], qrel_paths, path, '{0}-psg-tinybert-nrm-sum-{1}.run', rel_levels, 1, 10, 1)\n",
    "all_psg_dfs = load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, path, ['case-topics-psg-tinybert-nrm-'+str(y)+'-'+x+'.run' for x in  ['gauss', 'inv', 'exp', 'wind'] for y in  [2, 5, 10, 20, 50, 75, 100, 150, 200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tune_1d_with_ticks(names, metric_names, dfs, ticks, ylims=[], styles=[], legend_x=0.95, legend_y=0.3): \n",
    "    r = 1\n",
    "    c = 2\n",
    "\n",
    "    # r = int(len(metric_names)/2)\n",
    "    # c = r \n",
    "    # if c == r:\n",
    "    #     r-=1\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    fig.set_size_inches(16, 6)\n",
    "    cnt = 0 \n",
    "    x = [i for i in range(len(ticks))]\n",
    "    for m in metric_names:\n",
    "#         if m in metric_names: \n",
    "        for i, df in enumerate(dfs):\n",
    "            s = None\n",
    "            if i < len(styles):\n",
    "                s = styles[i]\n",
    "            axs[cnt].plot(x, [y[m] for y in df], linestyle=s)\n",
    "            if m.startswith('rbp@'):\n",
    "                es = 'rbp-res@'+m[4:]\n",
    "                axs[cnt].fill_between(x, [y[m] for y in df], [y[es]+y[m] for y in df], alpha=0.3)\n",
    "\n",
    "        axs[cnt].set_ylabel(metric_names[m],fontsize=18)\n",
    "\n",
    "        axs[cnt].tick_params(labelsize=12)\n",
    "        axs[cnt].yaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "        axs[cnt].set_xticks(x)\n",
    "        axs[cnt].set_xticklabels(ticks)\n",
    "        cnt += 1 \n",
    "        \n",
    "    for i in range(len(ylims)):\n",
    "        plt.gcf().get_axes()[i].set_ylim(ymax=ylims[i])\n",
    "    \n",
    "    if len(metric_names) % 2 != 0: \n",
    "        fig.delaxes(axs[row, -1])\n",
    "\n",
    "    fig.legend(names, bbox_to_anchor=[legend_x, legend_y], frameon=True, ncol=2, prop={\"size\": 15}).get_frame().set_edgecolor('black')\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_tune_1d_with_ticks(['base', 'gauss', 'inv', 'exp', 'wind'], RERANK_METRICS, [[base_df for x in range(9)]]+[all_dfs[i:i+9] for i in range(0, len(all_dfs), 9)], [2, 5, 10, 20, 50, 75, 100, 150, 200], styles=['--'], legend_y=0.26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('figures/bert-sen-wind.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plot_single_orientation(['base', 'sum-sen', 'sum-psg'], RERANK_METRICS, [[base_df for x in range(10)], sum_dfs[0], sum_psg_dfs[0]], 1, 10, 1, styles=['--'], legend=True, legend_y=0.35, legend_x=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2.savefig('figures/bert-sum.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_psg_fig = plot_tune_1d_with_ticks(['base', 'gauss', 'inv', 'exp', 'wind'], metrics, [[base_df for x in range(9)]]+[all_psg_dfs[i:i+9] for i in range(0, len(all_psg_dfs), 9)], [2, 5, 10, 20, 50, 75, 100, 150, 200], ylims=ylims, styles=['--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_psg_fig.savefig('figures/ausnl-bert-psg-wind.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "interped_dfs = []\n",
    "interped_wind_dfs = []\n",
    "interped_psg_dfs = []\n",
    "interped_psg_wind_dfs = []\n",
    "\n",
    "inter = Interpolater(os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior', 'case-topics-filtered-phrasestop-unigram_dir_mu_1050.00.run'))\n",
    "for _lambda in np.arange(0, 1.01, 0.01):\n",
    "    inter.interpolate(os.path.join(path, 'case-topics-tinybert-nrm-sum-5.run'), _lambda, 'tmp.run')\n",
    "    interped_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'])[0])\n",
    "    \n",
    "    inter.interpolate(os.path.join(path, 'case-topics-tinybert-nrm-100-wind.run'), _lambda, 'tmp.run')\n",
    "    interped_wind_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'])[0])\n",
    "    \n",
    "    inter.interpolate(os.path.join(path, 'case-topics-psg-tinybert-nrm-sum-9.run'), _lambda, 'tmp.run')\n",
    "    interped_psg_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'])[0])\n",
    "    \n",
    "    inter.interpolate(os.path.join(path, 'case-topics-psg-tinybert-nrm-75-inv.run'), _lambda, 'tmp.run')\n",
    "    interped_psg_wind_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plot_single_orientation(['DP', 'sen-sum', 'sen-wind', 'psg-sum', 'psg-inv'], RERANK_METRICS, \n",
    "        [[base_df for x in range(len(interped_dfs))], interped_dfs, interped_wind_dfs, interped_psg_dfs, interped_psg_wind_dfs], 0.0, 1.0, 0.01, orientation=0, legend=True, legend_x=0.82, legend_y=0.42, styles=['--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig3.savefig('figures/bert-interp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_qry = load_1d_dfs(['filtered-phrasestop'], [config.AUS_QREL_PATH], os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), 'case-topics-{0}-unigram_dir_mu_{1:.2f}.run', ['1'], 1050, 1050, 1, per_query=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_interp_df = select_1d_max_with_interp(['tinybert-nrm-sum-5', 'tinybert-nrm-100-wind', 'psg-tinybert-nrm-sum-9', 'psg-tinybert-nrm-75-inv'], [interped_dfs, interped_wind_dfs, interped_psg_dfs, interped_psg_wind_dfs], 0.0, 0.01, '$\\lambda$', inter, base_qry, base_df, 1050, os.path.join(path, 'case-topics-{0}.run'), config.AUS_QREL_PATH, '1', metrics=metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(max_interp_df.drop(['Unjudged@20'], axis='columns').to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation max results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_folds = read_folds('ausnl-folds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interped_dfs = []\n",
    "interped_wind_dfs = []\n",
    "interped_psg_dfs = []\n",
    "interped_psg_wind_dfs = []\n",
    "\n",
    "inter = Interpolater(os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior', 'case-topics-filtered-phrasestop-unigram_dir_mu_1050.00.run'))\n",
    "for _lambda in np.arange(0, 1.01, 0.01):\n",
    "    inter.interpolate(os.path.join(path, 'case-topics-tinybert-nrm-sum-5.run'), _lambda, 'tmp.run')\n",
    "    interped_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'], per_query=True)[0])\n",
    "    \n",
    "    inter.interpolate(os.path.join(path, 'case-topics-tinybert-nrm-100-wind.run'), _lambda, 'tmp.run')\n",
    "    interped_wind_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'], per_query=True)[0])\n",
    "    \n",
    "    inter.interpolate(os.path.join(path, 'case-topics-psg-tinybert-nrm-sum-9.run'), _lambda, 'tmp.run')\n",
    "    interped_psg_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'], per_query=True)[0])\n",
    "    \n",
    "    inter.interpolate(os.path.join(path, 'case-topics-psg-tinybert-nrm-75-inv.run'), _lambda, 'tmp.run')\n",
    "    interped_psg_wind_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'], per_query=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(columns=config.METRIC_NAMES)\n",
    "\n",
    "for ab, runs in zip(['tinybert-nrm-sum-5', 'tinybert-nrm-100-wind', 'psg-tinybert-nrm-sum-9', 'psg-tinybert-nrm-75-inv'],[interped_dfs, interped_wind_dfs, interped_psg_dfs, interped_psg_wind_dfs]):\n",
    "    cross = cross_validation(runs, tt_folds, config.METRIC_NAMES, base_qry)\n",
    "    cv_df.loc[ab] = cross[0]\n",
    "    \n",
    "cv_df.loc['$R$'] = ['{:.4f}'.format(base_df[m]) for m in config.METRIC_NAMES]\n",
    "cv_df = cv_df.rename(index={'tinybert-nrm-sum-5': 'sen-sum', \n",
    "                            'tinybert-nrm-100-wind': 'sen-wind', \n",
    "                            'psg-tinybert-nrm-sum-9': 'psg-sum', \n",
    "                            'psg-tinybert-nrm-75-inv': 'psg-inv'})\n",
    "cv_df = cv_df.reindex(['$R$', 'sen-sum', 'sen-wind', 'psg-sum', 'psg-inv'])\n",
    "write_table('tables/ausnl-bert', bold_max(cv_df).drop(['unjudged@20', 'recall_100'],axis='columns').rename(config.METRIC_NAMES, axis='columns').to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame()\n",
    "\n",
    "\n",
    "cv_res = cross_validation(interped_dfs, tt_folds, config.METRIC_NAMES, base_qry)[2]\n",
    "for c, m in zip(cv_res, config.METRIC_NAMES):\n",
    "    a[m] = c\n",
    "    \n",
    "qry_comp_df = a-base_qry\n",
    "om = copy.copy(metrics)\n",
    "del om['unjudged@20']\n",
    "qry_comp_fig = qry_comp_df[om.keys()].rename(metrics, axis='columns').plot.box(fontsize=15, boxprops=dict(linestyle='-', linewidth=2), medianprops=dict(linestyle='-', linewidth=2), color=dict(boxes='black', whiskers='black', medians='b', caps='r'),figsize=(16, 4)).axhline(y=0, xmin=0.0, xmax=1.0, linestyle='--', linewidth=1.0, color='grey')\n",
    "# qry_comp_fig.get_figure().savefig('figures/ausnl-bert-cv-dist.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upperbound calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# change this so i can provide a list of runs or a list of dfs ...\n",
    "# give it dfs for interped stuff \n",
    "# also provide list of run configs for each \n",
    "# so these can be read back at the end \n",
    "class UpperBound:\n",
    "    \n",
    "    def __init__(self, runs: List[str], path: str, qrel_path: str, rel_level: str):\n",
    "        self._runs = runs \n",
    "        self._path = path\n",
    "        self._qrel_path = qrel_path\n",
    "        self._rel_level = rel_level\n",
    "        \n",
    "        self._dfs = load_dfs(self._qrel_path, self._rel_level, self._path, self._runs, per_query=True)\n",
    "        \n",
    "    def calc_upperbound(self):\n",
    "        return pd.concat(self._dfs).fillna(0).max(level=0)\n",
    "    \n",
    "    def construct_upperbound_stat_sig(self, comp_df, name: str): \n",
    "\n",
    "        n = len(self._runs)\n",
    "        \n",
    "        cols = self._dfs[0].columns\n",
    "        query_maxes = []\n",
    "        for c in cols:\n",
    "            query_maxes.append({})\n",
    "\n",
    "        for row in self._dfs[0].iterrows():\n",
    "            val = row[1].to_dict()\n",
    "            for i, v in enumerate(val.values()):\n",
    "                query_maxes[i][row[0]] = (0, v)\n",
    "                \n",
    "        for i in range(1, n):\n",
    "            for row in self._dfs[i].iterrows():\n",
    "                larger = self._dfs[0].loc[row[0]].ge(row[1])\n",
    "                for ind, (l, val) in enumerate(zip(larger.items(), row[1])):\n",
    "                    if l: \n",
    "                        query_maxes[ind][row[0]] = (i, val)\n",
    "        \n",
    "        df = pd.DataFrame(self.calc_upperbound().mean())\n",
    "        for i, col in enumerate(cols):\n",
    "            v = float(df.loc[col])\n",
    "            sig = stats.ttest_rel(comp_df[col].fillna(0).values, pd.DataFrame(query_maxes[i]).fillna(0).T[1].values).pvalue\n",
    "            if sig < 0.01:\n",
    "                df.loc[col] = \"{:.4f}\".format(v) + \"$^{**}$\"\n",
    "            elif sig < 0.05:\n",
    "                df.loc[col] = \"{:.4f}\".format(v) + \"$^{*}$\"\n",
    "                \n",
    "        df = df.T\n",
    "        df.index = [name]\n",
    "        return df \n",
    "    \n",
    "def upperbound_runs(runs: List[pd.DataFrame]): \n",
    "\n",
    "    n = len(runs)\n",
    "\n",
    "    cols = runs[0].columns\n",
    "    query_maxes = []\n",
    "    for c in cols:\n",
    "        query_maxes.append({})\n",
    "\n",
    "    for i in range(n):\n",
    "        for row in runs[i].iterrows():\n",
    "            for v, (metric, score) in enumerate(row[1].iteritems()):\n",
    "                s = query_maxes[v].get(row[0], None)\n",
    "                if s is None or s[1] < score:\n",
    "                    query_maxes[v][row[0]] = (i, score, row[1]['unjudged@20'])\n",
    "        \n",
    "    return query_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub = UpperBound(['case-topics-tinybert-nrm-sum-{0}.run'.format(x) for x in range(1, 11)], path, config.AUS_QREL_PATH, config.AUS_REL_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate and compute all possible upperbound for runs\n",
    "\n",
    "names = ['sen-sum', 'sen-wind', 'psg-sum', 'psg-inv']\n",
    "configs = [\n",
    "    ['case-topics-tinybert-nrm-sum-{0}.run'.format(x) for x in range(1, 11, 1)],\n",
    "    ['case-topics-tinybert-nrm-'+str(y)+'-wind.run' for y in  [2, 5, 10, 20, 50, 75, 100, 150, 200]],\n",
    "    ['case-topics-psg-tinybert-nrm-sum-{0}.run'.format(x) for x in range(1, 11, 1)],\n",
    "    ['case-topics-psg-tinybert-nrm-'+str(y)+'-inv.run' for y in  [2, 5, 10, 20, 50, 75, 100, 150, 200]]\n",
    "]\n",
    "\n",
    "inter = Interpolater(os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior', 'case-topics-filtered-phrasestop-unigram_dir_mu_1050.00.run'))\n",
    "base_qry = load_1d_dfs(['filtered-phrasestop'], qrel_paths, os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), 'case-topics-{0}-unigram_dir_mu_{1:.2f}.run', rel_levels, 1050, 1050, 1, per_query=True)[0][0]\n",
    "\n",
    "ind = pd.MultiIndex.from_product([base_qry.columns, ['score', 'unjudged']])\n",
    "\n",
    "out_df_list = []\n",
    "\n",
    "for i, name in enumerate(names): \n",
    "    \n",
    "    interp_runs = []\n",
    "    interp_config = []\n",
    "    \n",
    "    for i, r in enumerate(configs[i]):\n",
    "        for _lambda in np.arange(0, 1.01, 0.01):\n",
    "            inter.interpolate(os.path.join(path, r), _lambda, 'tmp.run')\n",
    "            interp_runs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'], per_query=True)[0])\n",
    "            interp_config.append((i, _lambda))\n",
    "\n",
    "    maxes = upperbound_runs(interp_runs)\n",
    "\n",
    "    all_m_dfs = []\n",
    "    for i, x in enumerate(maxes):\n",
    "        tmp = pd.DataFrame.from_dict(x).T\n",
    "        sig = stats.ttest_rel(base_qry[base_qry.columns[i]].fillna(0).values, tmp[1].fillna(0).values).pvalue\n",
    "        m = tmp.mean()\n",
    "        if sig < 0.01:\n",
    "            m.loc[1] = \"{:.4f}\".format(m.loc[1]) + \"$^{**}$\"\n",
    "        elif sig < 0.05:\n",
    "            m.loc[1] = \"{:.4f}\".format(m.loc[1]) + \"$^{*}$\"\n",
    "        else: \n",
    "            m.loc[1] = '{0:.4f}'.format(m.loc[1])\n",
    "\n",
    "        m.index=['lambda', 'score', 'unjudged']\n",
    "\n",
    "        all_m_dfs.append((name, 'score', base_qry.columns[i], m['score']))\n",
    "        all_m_dfs.append((name, 'unjudged', base_qry.columns[i], m['unjudged']))\n",
    "\n",
    "    df = pd.DataFrame(all_m_dfs, index=ind)\n",
    "    df.columns=['run', '', 'metric', name]\n",
    "    out_df_list.append(df[name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base_df.loc['unjudged@20'])\n",
    "\n",
    "ind = pd.MultiIndex.from_product([base_df.index, ['score', 'unjudged']])\n",
    "tmp_df_ls = []\n",
    "for i, v in base_df.iteritems(): \n",
    "    tmp_df_ls.append(('$R$', 'score', i, '{0:.4f}'.format(v)))\n",
    "    tmp_df_ls.append(('$R$', 'unjudged', i, '{0:.4f}'.format(base_df.loc['unjudged@20'])))\n",
    "\n",
    "tmp_df = pd.DataFrame(tmp_df_ls, index=ind, columns=['run', '', 'metric', 'base'])\n",
    "tmp_df.columns=['run', '', 'metric', '$R$']\n",
    "\n",
    "# ind = pd.MultiIndex.from_product([base_df.index, ['score', 'unjudged']])\n",
    "# tmp_df_ls = []\n",
    "# for n, (i, v) in enumerate(dir_ub.calc_upperbound().iteritems()): \n",
    "#     tmp = pd.DataFrame.from_dict(v)\n",
    "#     sig = stats.ttest_rel(base_qry[base_qry.columns[n]].fillna(0).values, v.fillna(0).values).pvalue\n",
    "#     m = tmp.mean()\n",
    "#     print(m)\n",
    "#     if sig < 0.01:\n",
    "#         m.loc[1] = \"{:.4f}\".format(m.loc[1]) + \"$^{**}$\"\n",
    "#     elif sig < 0.05:\n",
    "#         m.loc[1] = \"{:.4f}\".format(m.loc[1]) + \"$^{*}$\"\n",
    "#     else: \n",
    "#         m.loc[1] = '{0:.4f}'.format(m.loc[1])\n",
    "\n",
    "#     m.index=['lambda', 'score', 'unjudged']\n",
    "    \n",
    "#     tmp_df_ls.append(('$R$\\subscript{upperbound}', 'score', i, m['score']))\n",
    "#     tmp_df_ls.append(('$R$\\subscript{upperbound}', 'unjudged', i, m['unjudged']))\n",
    "\n",
    "# ub_df = pd.DataFrame(tmp_df_ls, index=ind, columns=['run', '', 'metric', 'base'])\n",
    "# ub_df.columns=['run', '', 'metric', '$R$\\subscript{upperbound}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upperbound of $K$ for selecting top-k passages or window size\n",
    "\n",
    "df = pd.DataFrame([tmp_df['$R$']] + out_df_list)\n",
    "# df.index = names\n",
    "df = df.round(4)\n",
    "df = df.T.loc[list(RERANK_METRICS.keys())]\n",
    "df = pd.DataFrame(df, index=pd.MultiIndex.from_product([['recip_rank', 'err@20', 'recall_20', 'ndcg', 'rbp@0.80'], ['score', 'unjudged']]))\n",
    "df.index.set_levels(['-', 'unjudged'], level=1, inplace=True)\n",
    "print(bold_max(df.T).to_latex(escape=False))\n",
    "# write_table('tables/ausnl-bert-ub', bold_max(df.T).to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ub = UpperBound(['case-topics-filtered-phrasestop-unigram_dir_mu_{:.2f}.run'.format(x) for x in range(300, 3050, 50)], os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), config.AUS_QREL_PATH, config.AUS_REL_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_files = []\n",
    "run_names = ['filtered-phrasestop-unigram_dir_mu_1050.00.run', 'tinybert-nrm-sum-6.run', 'tinybert-nrm-100-wind.run', 'tinybert-nrm-50-exp.run', 'tinybert-nrm-50-inv.run', 'tinybert-nrm-2-gauss.run',]\n",
    "names = ['dirichlet_prior', 'sum', 'wind', 'inv', 'exp', 'gauss']\n",
    "dirs = [os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), path, path, path, path, path]\n",
    "for i, r in enumerate(run_names):\n",
    "    ev_files.append((names[i], os.path.join(dirs[i], 'case-topics-'+r).format('flattened'), None))\n",
    "\n",
    "ub_df = compute_stat_sig(ev_files, config.AUS_QREL_PATH, ['dirichlet_prior'], config.METRIC_NAMES, rel_level='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate and compute all possible upperbound for runs\n",
    "\n",
    "def make_upperbound(base_qry):\n",
    "    names = [y+x for y in ['sen-', 'psg-'] for x in ['gauss', 'inv', 'exp', 'wind']]\n",
    "    names += ['sen-sum', 'psg-sum', 'base-ub']\n",
    "    configs = ['case-topics-tinybert-nrm-'+str(y)+'-{0}.run'.format(x) for x in ['gauss', 'inv', 'exp', 'wind'] for y in [2, 5, 10, 20, 50, 75, 100, 150, 200]]\n",
    "    configs += ['case-topics-psg-tinybert-nrm-'+str(y)+'-{0}.run'.format(x) for x in ['gauss', 'inv', 'exp', 'wind'] for y in [2, 5, 10, 20, 50, 75, 100, 150, 200]]\n",
    "    configs = [configs[i:i+9] for i in range(0, len(configs), 9)]\n",
    "    configs += [['case-topics-tinybert-nrm-sum-{0}.run'.format(x) for x in range(1, 11, 1)]]\n",
    "    configs += [['case-topics-psg-tinybert-nrm-sum-{0}.run'.format(x) for x in range(1, 11, 1)]]\n",
    "    configs += [['case-topics-filtered-phrasestop-unigram_dir_mu_{0:.2f}.run'.format(x) for x in range(300, 3050, 50)]]\n",
    "    \n",
    "    paths = [path]*(len(names)-1)\n",
    "    paths.append(os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'))\n",
    "\n",
    "    ind = pd.MultiIndex.from_product([base_qry.columns, ['score', 'unjudged']])\n",
    "    \n",
    "    out_df_list = []\n",
    "\n",
    "    for i, name in enumerate(names): \n",
    "\n",
    "        runs = load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, paths[i], configs[i], per_query=True)\n",
    "        maxes = upperbound_runs(runs)\n",
    "\n",
    "        all_m_dfs = []\n",
    "        for j, x in enumerate(maxes):\n",
    "            tmp = pd.DataFrame.from_dict(x).T\n",
    "            sig = stats.ttest_rel(base_qry[base_qry.columns[j]].fillna(0).values, tmp[1].fillna(0).values).pvalue\n",
    "            m = tmp.mean()\n",
    "            if sig < 0.01:\n",
    "                m.loc[1] = \"{:.4f}\".format(m.loc[1]) + \"$^{**}$\"\n",
    "            elif sig < 0.05:\n",
    "                m.loc[1] = \"{:.4f}\".format(m.loc[1]) + \"$^{*}$\"\n",
    "            else: \n",
    "                m.loc[1] = '{0:.4f}'.format(m.loc[1])\n",
    "\n",
    "            m.index=['lambda', 'score', 'unjudged']\n",
    "\n",
    "            all_m_dfs.append((name, 'score', base_qry.columns[j], m['score']))\n",
    "            all_m_dfs.append((name, 'unjudged', base_qry.columns[j], m['unjudged']))\n",
    "\n",
    "        df = pd.DataFrame(all_m_dfs, index=ind)\n",
    "        df.columns=['run', '', 'metric', name]\n",
    "        out_df_list.append(df[name])\n",
    "        \n",
    "    return out_df_list\n",
    "\n",
    "ub_no_interp = make_upperbound(base_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.MultiIndex.from_product([base_df.index, ['score', 'unjudged']])\n",
    "tmp_df_ls = []\n",
    "for i, v in base_df.iteritems(): \n",
    "    tmp_df_ls.append((name, 'score', i, v))\n",
    "    tmp_df_ls.append((name, 'unjudged', i, base_df.loc['unjudged@20']))\n",
    "\n",
    "tmp_df = pd.DataFrame(tmp_df_ls, index=ind, columns=['run', '', 'metric', 'base'])\n",
    "tmp_df['base']\n",
    "ub_no_interp.append(tmp_df['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub_no_interp = ub_no_interp[:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upperbound for just selecting k \n",
    "ub_no_df = pd.DataFrame(ub_no_interp)\n",
    "# df.index = names\n",
    "ub_no_df = ub_no_df.round(4)\n",
    "ub_no_df = ub_no_df.T.loc[list(RERANK_METRICS.keys())]\n",
    "ub_no_df = pd.DataFrame(ub_no_df, index=pd.MultiIndex.from_product([['recip_rank', 'err@20', 'recall_20', 'ndcg', 'rbp@0.80'], ['score', 'unjudged']]))\n",
    "ub_no_df.index.set_levels(['-', 'unjudged'], level=1, inplace=True)\n",
    "ub_no_df.T\n",
    "# write_table('tables/ausnl-bert-k-ub', ub_no_df.T.to_latex(escape=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
