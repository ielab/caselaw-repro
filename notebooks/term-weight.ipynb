{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "nb_dir = os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from plotlib.loaders import *\n",
    "from plotlib.plotters import *\n",
    "\n",
    "from phdconf import stop\n",
    "from phdconf import config\n",
    "from phdconf.config import * \n",
    "\n",
    "from typing import List, Dict\n",
    "import math \n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(config.AUS_TOPIC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path: str):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "class EmbMethod:\n",
    "    SUM = 0\n",
    "    MEAN = 1 \n",
    "    CF_SUM = 2 \n",
    "    CF_MEAN = 3\n",
    "\n",
    "def embed(tokens: List[str], embeddings, dim: int=100, method: EmbMethod = EmbMethod.SUM, coll_stats = None) -> np.array:\n",
    "    e = []\n",
    "    if method > EmbMethod.MEAN: \n",
    "        for x in tokens: \n",
    "            if x in embeddings:\n",
    "                mul = coll_stats[x] if x in coll_stats else 1.0\n",
    "                e.append(mul * embeddings[x])\n",
    "    else: \n",
    "        e = [embeddings[x] for x in tokens if x in embeddings]\n",
    "    if len(e) == 0:\n",
    "        return np.zeros((dim,), dtype='float32')\n",
    "    \n",
    "    e = np.sum(e, axis=0)\n",
    "    if method == EmbMethod.MEAN or method == EmbMethod.CF_MEAN:\n",
    "        e /= float(len(tokens))\n",
    "            \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_vectors('../embeddings/filtered-100d.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_embeddings = load_vectors('../embeddings/para-phrase-100d.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = {\n",
    "    'first', \n",
    "    'second', \n",
    "    'third',\n",
    "    'fourth',\n",
    "    'fifth',\n",
    "    'sixth',\n",
    "    'seventh',\n",
    "    'eighth',\n",
    "    'ninth',\n",
    "    'tenth',\n",
    "    'eleventh',\n",
    "    'twelfth',\n",
    "    'thirteenth',\n",
    "    'fourteenth',\n",
    "    'fifteenth',\n",
    "    'sixteenth',\n",
    "    'seventeenth',\n",
    "    'eighteenth',\n",
    "    'nineteenth',\n",
    "    'twenty',\n",
    "    'thirty',\n",
    "    'fourty',\n",
    "}\n",
    "\n",
    "num_exclusion = {\n",
    "    'amendment',\n",
    "    'degree',\n",
    "    'refusal'\n",
    "}\n",
    "\n",
    "find2 = {\n",
    "    'north',\n",
    "    'south',\n",
    "    'east',\n",
    "    'west'\n",
    "}\n",
    "\n",
    "find3 = {\n",
    "    'set',\n",
    "    'hold',\n",
    "    'exemplary',\n",
    "    'intending',\n",
    "    'include',\n",
    "    'including',\n",
    "    'relies',\n",
    "    'relied',\n",
    "    'pursuant',\n",
    "    'thereto',\n",
    "    'behalf',\n",
    "    'giving',\n",
    "    'give',\n",
    "    'apparently',\n",
    "    'subsequently',\n",
    "    'jurisdictional',\n",
    "    'included',\n",
    "    'includes',\n",
    "    'appellant',\n",
    "    'respondent',\n",
    "    'resulting',\n",
    "    'such',\n",
    "    's',\n",
    "    'x',\n",
    "    't',\n",
    "    'th',\n",
    "    'thing',\n",
    "    're',\n",
    "    'subject'\n",
    "}\n",
    "\n",
    "find4 = {\n",
    "    'nonetheless',\n",
    "    'follows',\n",
    "    'referred',\n",
    "    'thereto',\n",
    "    'behalf',\n",
    "    'hastily',\n",
    "    'instance',\n",
    "    'instances',\n",
    "    'such',\n",
    "    's',\n",
    "    'applicant',\n",
    "    'applicants',\n",
    "    'respondent',\n",
    "    'respondents',\n",
    "    'th',\n",
    "    'x',\n",
    "    'such',\n",
    "    'll',\n",
    "    'only',\n",
    "    'way',\n",
    "    're',\n",
    "    'much',\n",
    "    'st',\n",
    "    'consideration',\n",
    "    'organisation'\n",
    "}\n",
    "\n",
    "#     consider 'made' and 'make' as exclusions\n",
    "\n",
    "def filter_phrases(phrases, splitter=' '):\n",
    "    d = isinstance(phrases, dict)\n",
    "    if d:\n",
    "        ret = {}\n",
    "    else:\n",
    "        ret = []\n",
    "    for phrase in phrases:\n",
    "        toks = phrase.split(splitter)\n",
    "        if len(toks) < 2:\n",
    "            continue \n",
    "        \n",
    "        cont = False \n",
    "        cnt = 0\n",
    "\n",
    "        if toks[-1] in find3 or len(toks[-1]) == 1:\n",
    "            continue\n",
    "        \n",
    "        if len(toks) == 2 and toks[0].startswith('claim'):\n",
    "            continue\n",
    "            \n",
    "        if len(toks) == 3 and toks[0] == 'intention' and toks[1] != 'to':\n",
    "            continue\n",
    "            \n",
    "        if toks[0] == 'intention' and len(toks) == 2:\n",
    "            continue\n",
    "            \n",
    "        if toks[0] in find4 or len(toks[0]) == 1:\n",
    "            continue\n",
    "            \n",
    "        unique = set()\n",
    "            \n",
    "        num = False \n",
    "        num_ex = False\n",
    "        for t, tok in enumerate(toks):\n",
    "            unique.add(tok)\n",
    "            \n",
    "            if tok in find: \n",
    "                num = True \n",
    "            if tok in num_exclusion:\n",
    "                num_ex = True\n",
    "            if tok in find2:\n",
    "                cnt += 1 \n",
    "            if tok == 'nunc' and t != 0:\n",
    "                cont = True \n",
    "                break\n",
    "                \n",
    "        if num and not num_ex: \n",
    "            continue\n",
    "        \n",
    "        if len(toks) > len(unique): \n",
    "            continue\n",
    "    \n",
    "        if cont or cnt > 1:\n",
    "            continue \n",
    "            \n",
    "        if d: \n",
    "            ret[phrase] = phrases[phrase]\n",
    "        else:\n",
    "            ret.append(phrase)\n",
    "        \n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coll_stats(path: str, total=None) -> Dict[str, float]:\n",
    "    stats = {}\n",
    "    \n",
    "    idf = total is None\n",
    "    if idf:\n",
    "        total = 0.0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            if idf: \n",
    "                v = float(parts[-1])\n",
    "                total += v\n",
    "                stats[' '.join(parts[:-1])] = v\n",
    "            else:\n",
    "                stats[' '.join(parts[:-1])] = math.log(total / float(parts[-1])+1.0) + 1.0\n",
    "    \n",
    "    if idf: \n",
    "        for k, v in stats.items():\n",
    "            stats[k] = math.log(total / v+1.0) + 1.0\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_stats = load_coll_stats('../features/filtered-stop-top-tokens.txt')\n",
    "idf_stats = load_coll_stats('../features/filtered-stop-idf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_embeddings = filter_phrases(phrase_embeddings, '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in phrase_embeddings if x.endswith('_date')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(tokens: List[str], n: int): \n",
    "    ret = []\n",
    "    for i in range(0, len(tokens)-(n-1)):\n",
    "           ret.append('_'.join(tokens[i:i+n]))\n",
    "           \n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg(terms, ictf_lookup, idf_lookup):\n",
    "    ictfs = sum([ictf_lookup[x] for x in terms])\n",
    "    idfs = sum([idf_lookup[x] for x in terms])\n",
    "    \n",
    "    return ictfs / float(len(terms)), idfs / float(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "for topic in queries.values(): \n",
    "    print(topic['topic'])\n",
    "    tokens = topic['topic'].lower().replace(',', '').replace('?', '').replace('\\'', '').replace('/', '').replace('â€™', '').split()\n",
    "    tokens = [x for x in tokens if x not in stop.stop]\n",
    "    diff_tokens = copy.copy(tokens)\n",
    "    keep = {}\n",
    "    for grams in [n_gram(tokens, 2), n_gram(tokens, 3)]:\n",
    "        for i, gram in enumerate(grams):\n",
    "            if gram in phrase_embeddings:\n",
    "                ts = gram.split('_')\n",
    "                vals = get_avg(ts, coll_stats, idf_stats)\n",
    "                keep[gram] = (i, len(ts), vals[0], vals[1])\n",
    "    for meth in [EmbMethod.SUM, EmbMethod.CF_SUM]:\n",
    "        qry_vec = embed(tokens, embeddings, method=meth, coll_stats=idf_stats).reshape(1, -1)\n",
    "        tok_vecs = [embed([tok], embeddings, method=meth, coll_stats=idf_stats).reshape(1, -1) for tok in tokens]\n",
    "        sims = [(tokens[i], cosine_similarity(tok_vecs[i], qry_vec)[0][0], idf_stats[tokens[i]]) for i in range(len(tokens))]\n",
    "        print(sorted(sims, key= lambda x: x[1], reverse=True))\n",
    "    \n",
    "    print(tokens)\n",
    "    keep = OrderedDict(sorted(keep.items(), key=lambda t: (t[1][0], -t[1][1])))\n",
    "    print(keep)\n",
    "    print('-'*40)\n",
    "    if cnt > 10:\n",
    "        break\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
