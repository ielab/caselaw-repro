{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase based filtering results. Considers impact on set based measures and ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at phrase boolean queries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys \n",
    "import os \n",
    "\n",
    "nb_dir = os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from plotlib.loaders import *\n",
    "from plotlib.plotters import *\n",
    "\n",
    "from phdconf import config\n",
    "from phdconf.config import * \n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(config.AUS_TOPIC_PATH)\n",
    "broad, specific = load_query_types(queries)\n",
    "law, fact, generic = load_query_focus_types(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = ['filtered-phrasestop']#, config.SIGIR_INDEX_NAME]\n",
    "qrel_paths = [config.AUS_QREL_PATH]#, config.SIGIR_QREL_PATH]\n",
    "rel_levels = [config.AUS_REL_LEVEL]#, config.SIGIR_REL_LEVEL]\n",
    "display_names = ['AUS']#, 'SIGIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = load_1d_dfs(index_names, qrel_paths, os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), 'case-topics-{0}-unigram_dir_mu_{1:.2f}.run', rel_levels, 1050, 1050, 1)[0][0]\n",
    "base_qry = load_1d_dfs(index_names, qrel_paths, os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior'), 'case-topics-{0}-unigram_dir_mu_{1:.2f}.run', rel_levels, 1050, 1050, 1, per_query=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_qry_set = load_1d_dfs(index_names, qrel_paths, os.path.join(BASE_DIR, 'dirichlet_prior'), 'case-topics-{0}-unigram_dir_mu_{1:.2f}.run', rel_levels, 1050, 1050, 1, per_query=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_per_qry(path:str):\n",
    "    res = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            if parts[0] not in res:\n",
    "                res[parts[0]] = 0\n",
    "                \n",
    "            res[parts[0]] += 1\n",
    "            \n",
    "    return res\n",
    "\n",
    "set_res = res_per_qry(os.path.join(BASE_DIR, 'dp-set', 'case-topics-filtered-phrasestop-unigram_dir_mu_1050.00.run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(list(set_res.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(list(res_per_qry(os.path.join(BASE_DIR, 'phrases', 'phrase-all-or.run')).values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_queries = [7, 24, 57, 70, 76, 85, 96]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = ['phrase-all-or.run', 'phrase-all-and.run', 'phrase-or.run', 'phrase-and.run']\n",
    "names = ['all-or', 'all-and', 'or', 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qry = load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, os.path.join(BASE_DIR, 'phrases'), [runs[0]], per_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = all_qry[0]-base_qry\n",
    "metrics = copy.copy(config.METRIC_NAMES)\n",
    "del metrics['unjudged@20']\n",
    "fig = diff[metrics].rename(metrics, axis='columns').plot.box(fontsize=15, boxprops=dict(linestyle='-', linewidth=2), color=dict(boxes='black', whiskers='black', caps='r'), medianprops=dict(linestyle='-', linewidth=2, color='b'), figsize=(16, 4)).axhline(y=0, xmin=0.0, xmax=1.0, linestyle='--', linewidth=1.0, color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.get_figure().savefig('figures/ausnl-all-or-phrase-qry-comp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = all_qry[0]-base_qry\n",
    "diff2 = diff2[diff2.index.isin(small_queries)]\n",
    "fig = diff2[metrics].rename(metrics, axis='columns').plot.box(fontsize=15, boxprops=dict(linestyle='-', linewidth=2), color=dict(boxes='black', whiskers='black', caps='r'), medianprops=dict(linestyle='-', linewidth=2, color='b'), figsize=(16, 4)).axhline(y=0, xmin=0.0, xmax=1.0, linestyle='--', linewidth=1.0, color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2[metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_qry[base_qry.index.isin(small_queries)][metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table('tables/ausnl-phrase-qry-diff', diff2[metrics].rename(columns=metrics).round(4).to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.get_figure().savefig('figures/ausnl-all-or-phrase-qry-comp-small-queries.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, os.path.join(BASE_DIR, 'phrases'), runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dfs+[base_df], index=names+['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_files = [('$R$', os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior', 'case-topics-filtered-phrasestop-unigram_dir_mu_1050.00.run'), None)]\n",
    "ev_files.append(('$R_{all}$', os.path.join(BASE_DIR, 'dirichlet_prior', 'case-topics-filtered-phrasestop-unigram_dir_mu_1050.00.run'), None))\n",
    "for i, r in enumerate(runs):\n",
    "    ev_files.append((names[i], os.path.join(BASE_DIR, 'phrases', r), None))\n",
    "\n",
    "metrics = copy.copy(config.METRIC_NAMES)\n",
    "metrics['set_P'] = 'P'\n",
    "metrics['set_recall'] = 'R'\n",
    "metrics['set_F_.5'] = 'F.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_stat_sig(ev_files, config.AUS_QREL_PATH, ['$R$', '$R_{all}$'], metrics, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table('tables/ausnl-phrase-filter', bold_max(df[metrics]).rename(columns=metrics).round(4).to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = Interpolater(os.path.join(BASE_DIR, 'preprocessing', 'dirichlet_prior', 'case-topics-filtered-phrasestop-unigram_dir_mu_1050.00.run'), normalize=True)\n",
    "\n",
    "vals = [0, 300, 1050, 3000]\n",
    "dfs = []\n",
    "\n",
    "for v in vals:\n",
    "    interped_dfs = []\n",
    "    for _lambda in np.arange(0, 1.0, 0.01):\n",
    "        inter.interpolate(os.path.join(BASE_DIR, 'phrases', 'case-topics-phrase-scoring-mu-{0:.2f}.run'.format(v)), _lambda, 'tmp.run')\n",
    "        interped_dfs.append(load_dfs(config.AUS_QREL_PATH, config.AUS_REL_LEVEL, '', ['tmp.run'])[0])\n",
    "    dfs.append(interped_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = 50\n",
    "metrics = copy.copy(config.METRIC_NAMES)\n",
    "del metrics['recall_100']\n",
    "\n",
    "phrase_interp_fig = plot_tune_1d_comp(['base', 'mle', 'mu=300', 'mu=1050', 'mu=3000'], RERANK_METRICS, [[base_df for x in range(to+1)]] + [x[:to+1] for x in dfs], 0.00, (to)/100, 0.01, legend_x=0.965, ylims=RERANK_YLIMS, styles=['--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified so does stat sig to not 0 \n",
    "def select_1d_max_with_interp(display_names, dfs, start, increment, name, interp, base_qry, base_df, base_val, path, qrel_path, rel_level, metrics=None):\n",
    "    measure_max = {}\n",
    "    for i in range(len(display_names)):\n",
    "        for j in range(1, len(dfs[i])):\n",
    "            for m in dfs[i][j].index:\n",
    "                if m not in metrics: \n",
    "                    continue \n",
    "                val = dfs[i][j][m]\n",
    "                if (display_names[i], metrics[m]) not in measure_max: \n",
    "                    measure_max[(display_names[i], metrics[m])] = {'-': val, name: '{0:.2f}'.format(j*increment+start)}\n",
    "                else: \n",
    "                    if measure_max[(display_names[i], metrics[m])]['-'] < val:\n",
    "                        measure_max[(display_names[i], metrics[m])] = {'-': val, name: '{0:.2f}'.format(j*increment+start)}\n",
    "\n",
    "    back_metric = {v: k for k, v in metrics.items()}\n",
    "    for k, v in measure_max.items():\n",
    "        if k[1] == 'Unjudged@20':\n",
    "            continue\n",
    "        _l = float(v[name])\n",
    "        if _l == 0.00:\n",
    "            v['-'] = '{0:.4f}'.format(v['-'])\n",
    "        else:\n",
    "            print(path, k[0])\n",
    "            interp.interpolate(path.format(k[0]), _l, 'tmp.run')\n",
    "            comp = load_dfs(qrel_path, rel_level, '', ['tmp.run'], per_query=True)[0]\n",
    "            p = stats.ttest_rel(base_qry[back_metric[k[1]]], comp[back_metric[k[1]]]).pvalue\n",
    "            if p < 0.01:\n",
    "                v['-'] = '{0:.4f}'.format(v['-'])+'$^{**}$'\n",
    "            elif p < 0.05:\n",
    "                v['-'] = '{0:.4f}'.format(v['-'])+'$^{*}$'\n",
    "            else:\n",
    "                v['-'] = '{0:.4f}'.format(v['-'])\n",
    "                \n",
    "        \n",
    "    for x in base_df.items():\n",
    "        if x[0] not in metrics: \n",
    "            continue\n",
    "        measure_max[('base', metrics[x[0]])] = {'-': '{0:.4f}'.format(x[1]), name: '{0:.2f}'.format(base_val)}\n",
    "        \n",
    "    max_df = pd.DataFrame.from_dict(measure_max).stack().unstack(level=0)\n",
    "    return max_df.reindex(list(metrics.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_interp_max = select_1d_max_with_interp([300, 1050, 3000], dfs, 0.0, 0.01, '$\\lambda$', inter, base_qry, base_df, 1050, os.path.join(BASE_DIR,  'phrases', 'case-topics-phrase-scoring-mu-{0:.2f}.run'), config.AUS_QREL_PATH, config.AUS_REL_LEVEL, metrics=metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_interp_max"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
