{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotlib.loaders import *\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import annoy\n",
    "import nltk\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_phrase_scores(path: str, feature: str) -> Dict[str, Tuple[str, int]]: \n",
    "    ind = {\n",
    "        'recip_rank': 2,\n",
    "        'recall_20': 3,\n",
    "        'recall_100': 4,\n",
    "        'ndcg': 5,\n",
    "        'rbp@0.10': 6,\n",
    "        'rbp@0.50': 7,\n",
    "        'rbp@0.80': 8,\n",
    "    }[feature]\n",
    "    \n",
    "    out = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            if parts[0] not in out: \n",
    "                out[parts[0]] = []\n",
    "            v = float(parts[ind])\n",
    "            inc = 1 \n",
    "            if v < 0.0:\n",
    "                inc = -1 \n",
    "            elif v == 0.0:\n",
    "                inc = 0 \n",
    "            out[parts[0]].append((parts[1], inc))\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = load_queries('/home/danlocke/go/src/github.com/dan-locke/phd-data/case-topics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path: str):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "class EmbMethod:\n",
    "    SUM = 0\n",
    "    MEAN = 1 \n",
    "    CF_SUM = 2 \n",
    "    CF_MEAN = 3\n",
    "\n",
    "def embed(tokens: List[str], embeddings, dim: int=100, method: EmbMethod = EmbMethod.SUM, coll_stats = None) -> np.array:\n",
    "    e = []\n",
    "    if method > EmbMethod.MEAN: \n",
    "        for x in tokens: \n",
    "            if x in embeddings:\n",
    "                mul = coll_stats[x] if x in coll_stats else 1.0\n",
    "                e.append(mul * embeddings[x])\n",
    "    else: \n",
    "        e = [embeddings[x] for x in tokens if x in embeddings]\n",
    "    if len(e) == 0:\n",
    "        return np.zeros((dim,), dtype='float32')\n",
    "    \n",
    "    e = np.sum(e, axis=0)\n",
    "    if method == EmbMethod.MEAN or method == EmbMethod.CF_MEAN:\n",
    "        e /= len(tokens)\n",
    "            \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = {\n",
    "    'first', \n",
    "    'second', \n",
    "    'third',\n",
    "    'fourth',\n",
    "    'fifth',\n",
    "    'sixth',\n",
    "    'seventh',\n",
    "    'eighth',\n",
    "    'ninth',\n",
    "    'tenth',\n",
    "    'eleventh',\n",
    "    'twelfth',\n",
    "    'thirteenth',\n",
    "    'fourteenth',\n",
    "    'fifteenth',\n",
    "    'sixteenth',\n",
    "    'seventeenth',\n",
    "    'eighteenth',\n",
    "    'nineteenth',\n",
    "    'twenty',\n",
    "    'thirty',\n",
    "    'fourty',\n",
    "}\n",
    "\n",
    "num_exclusion = {\n",
    "    'amendment',\n",
    "    'degree',\n",
    "    'refusal'\n",
    "}\n",
    "\n",
    "find2 = {\n",
    "    'north',\n",
    "    'south',\n",
    "    'east',\n",
    "    'west'\n",
    "}\n",
    "\n",
    "find3 = {\n",
    "    'set',\n",
    "    'hold',\n",
    "    'exemplary',\n",
    "    'intending',\n",
    "    'include',\n",
    "    'including',\n",
    "    'relies',\n",
    "    'relied',\n",
    "    'pursuant',\n",
    "    'thereto',\n",
    "    'behalf',\n",
    "    'giving',\n",
    "    'give',\n",
    "    'apparently',\n",
    "    'subsequently',\n",
    "    'jurisdictional',\n",
    "    'included',\n",
    "    'includes',\n",
    "    'appellant',\n",
    "    'respondent',\n",
    "    'resulting',\n",
    "    'such',\n",
    "    's',\n",
    "    'x',\n",
    "    't',\n",
    "    'th',\n",
    "    'thing',\n",
    "    're',\n",
    "    'subject'\n",
    "}\n",
    "\n",
    "find4 = {\n",
    "    'nonetheless',\n",
    "    'follows',\n",
    "    'referred',\n",
    "    'thereto',\n",
    "    'behalf',\n",
    "    'hastily',\n",
    "    'instance',\n",
    "    'instances',\n",
    "    'such',\n",
    "    's',\n",
    "    'applicant',\n",
    "    'applicants',\n",
    "    'respondent',\n",
    "    'respondents',\n",
    "    'th',\n",
    "    'x',\n",
    "    'such',\n",
    "    'll',\n",
    "    'only',\n",
    "    'way',\n",
    "    're',\n",
    "    'much',\n",
    "}\n",
    "\n",
    "#     consider 'made' and 'make' as exclusions\n",
    "\n",
    "def filter_phrases(phrases, splitter=' '):\n",
    "    d = isinstance(phrases, dict)\n",
    "    if d:\n",
    "        ret = {}\n",
    "    else:\n",
    "        ret = []\n",
    "    for phrase in phrases:\n",
    "        toks = phrase.split(splitter)\n",
    "        if len(toks) < 2:\n",
    "            continue \n",
    "        \n",
    "        cont = False \n",
    "        cnt = 0\n",
    "\n",
    "        if toks[-1] in find3 or len(toks[-1]) == 1:\n",
    "            continue\n",
    "        \n",
    "        if len(toks) == 2 and toks[0].startswith('claim'):\n",
    "            continue\n",
    "            \n",
    "        if len(toks) == 3 and toks[0] == 'intention' and toks[1] != 'to':\n",
    "            continue\n",
    "            \n",
    "        if toks[0] == 'intention' and len(toks) == 2:\n",
    "            continue\n",
    "            \n",
    "        if toks[0] in find4 or len(toks[0]) == 1:\n",
    "            continue\n",
    "            \n",
    "        unique = set()\n",
    "            \n",
    "        num = False \n",
    "        num_ex = False\n",
    "        for t, tok in enumerate(toks):\n",
    "            unique.add(tok)\n",
    "            \n",
    "            if tok in find: \n",
    "                num = True \n",
    "            if tok in num_exclusion:\n",
    "                num_ex = True\n",
    "            if tok in find2:\n",
    "                cnt += 1 \n",
    "            if tok == 'nunc' and t != 0:\n",
    "                cont = True \n",
    "                break\n",
    "                \n",
    "        if num and not num_ex: \n",
    "            continue\n",
    "        \n",
    "        if len(toks) > len(unique): \n",
    "            continue\n",
    "    \n",
    "        if cont or cnt > 1:\n",
    "            continue \n",
    "            \n",
    "        if d: \n",
    "            ret[phrase] = phrases[phrase]\n",
    "        else:\n",
    "            ret.append(phrase)\n",
    "        \n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    'the',\n",
    "    'of',\n",
    "    'to',\n",
    "    'in', \n",
    "    'for',\n",
    "    'that',\n",
    "    'and',\n",
    "    'on',\n",
    "    'is',\n",
    "    'be',\n",
    "    'by',\n",
    "    'a',\n",
    "    'an',\n",
    "    'was',\n",
    "    'it',\n",
    "    'as',\n",
    "    'this',\n",
    "    'which',\n",
    "    'with',\n",
    "    'have',\n",
    "    'at',\n",
    "    'been',\n",
    "    'there',\n",
    "    'no',\n",
    "    'or',\n",
    "    'from',\n",
    "    'has',\n",
    "    'any',\n",
    "    'i',\n",
    "    'would',\n",
    "    'were',\n",
    "    'had',\n",
    "    'are',\n",
    "    'if',\n",
    "    'also',\n",
    "    'before',\n",
    "    'but',\n",
    "    'his',\n",
    "    'other',\n",
    "    'those',\n",
    "    'so',\n",
    "    'he',\n",
    "    'did',\n",
    "    'its',\n",
    "    'she',\n",
    "    'hers',\n",
    "    'their',\n",
    "    'theirs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_vectors('/home/danlocke/fastText/filtered-100d.vec')\n",
    "embeddings = {k: v for k, v in embeddings.items() if k.islower() and k.isalpha()}\n",
    "term_embeddings = [x for x in embeddings.values()]\n",
    "term_lookup = [x for x in embeddings.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_embeddings = load_vectors('/home/danlocke/fastText/para-phrase-100d.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_embeddings = filter_phrases(phrase_embeddings, '_')\n",
    "phrase_embeddings_vecs = [x for x in phrase_embeddings.values()]\n",
    "phrase_lookup = [x for x in phrase_embeddings.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_phrase_scores('expansion-changes.txt', 'ndcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(tokens: List[str], n: int): \n",
    "    ret = []\n",
    "    for i in range(0, len(tokens)-(n-1)):\n",
    "           ret.append('_'.join(tokens[i:i+n]))\n",
    "           \n",
    "    return ret \n",
    "\n",
    "def get_phrases(text: str, phrase_lookup: Dict[str, np.array]): \n",
    "    found = {}\n",
    "    tokens = [x for x in text.lower().replace(',', '').replace('?', '').replace('\\'', '').replace('/', '').replace('’', '').split() if x not in stop]\n",
    "    for grams in [n_gram(tokens, 2), n_gram(tokens, 3)]:\n",
    "        for gram in grams:\n",
    "            if gram in phrase_lookup:\n",
    "                found[gram] = 1\n",
    "                \n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = annoy.AnnoyIndex(100, 'angular')\n",
    "for i, x in enumerate(phrase_embeddings_vecs):\n",
    "    ind.add_item(i, x) \n",
    "ind.build(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_phrases = {}\n",
    "for topic in topics:\n",
    "    phrases = get_phrases(topics[topic]['topic'], phrase_embeddings)\n",
    "    phrases = [(x, phrase_embeddings[x], [phrase_lookup[y] for y in ind.get_nns_by_vector(phrase_embeddings[x], 25)]) for x in phrases]\n",
    "    qry_phrases[topic] = phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "def features(topic, phrases, qry_phrases):\n",
    "    out = []\n",
    "    topic = int(topic)\n",
    "    qry_toks = [x for x in topics[topic]['topic'].lower().replace(',', '').replace('?', '').replace('\\'', '').replace('/', '').replace('’', '').split() if x not in stop]\n",
    "    qry_vec = embed(qry_toks, embeddings).reshape(1, -1)\n",
    "    for phrase_tuple in phrases:\n",
    "        phrase_vec = phrase_embeddings[phrase_tuple[0]].reshape(1, -1)\n",
    "        \n",
    "        phrase_toks = set(phrase_tuple[0].split('_'))\n",
    "        \n",
    "        phrase_tok_vec = embed(phrase_toks, embeddings).reshape(1, -1)\n",
    "        matched_vec = None \n",
    "        matched_tok_vec = None \n",
    "        diff_toks_vec = None \n",
    "        num_diff_toks = 0\n",
    "        num_phrase_toks = len(phrase_toks)\n",
    "        orig_phrase = None \n",
    "        \n",
    "        min_qry_phrase_sim = None\n",
    "        max_qry_phrase_sim = None \n",
    "        \n",
    "        num_top_match = 0 \n",
    "        \n",
    "        for x in qry_phrases[topic]:\n",
    "            if phrase_tuple[0] in x[2]:\n",
    "                num_top_match += 1 \n",
    "                    \n",
    "                # the phrase is a potential expansion of this original query phrase\n",
    "\n",
    "                orig_phrase = x[0]\n",
    "                matched_vec = phrase_embeddings[x[0]].reshape(1, -1)\n",
    "                matched_toks = set(x[0].split('_'))\n",
    "                matched_tok_vec = embed(matched_toks, embeddings).reshape(1, -1)\n",
    "                \n",
    "                if min_qry_phrase_sim is None: \n",
    "                    min_qry_phrase_sim = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                else:\n",
    "                    s = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                    if s > min_qry_phrase_sim:\n",
    "                        min_qry_phrase_sim = s \n",
    "                        \n",
    "                if max_qry_phrase_sim is None: \n",
    "                    max_qry_phrase_sim = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                else:\n",
    "                    s = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                    if s > max_qry_phrase_sim:\n",
    "                        max_qry_phrase_sim = s \n",
    "\n",
    "                diff_toks = matched_toks - phrase_toks\n",
    "                num_diff_toks = len(diff_toks)\n",
    "                diff_toks_vec = embed(diff_toks, embeddings).reshape(1, -1)\n",
    "             \n",
    "        num_top_match = float(num_top_match) / float(len(qry_phrases[topic]))\n",
    "                \n",
    "        phrase_exp_sim = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "        phrase_exp_token_sim = cosine_similarity(phrase_tok_vec, matched_tok_vec)[0][0]\n",
    "        qry_exp_sim = cosine_similarity(qry_vec, phrase_tok_vec)[0][0]\n",
    "        diff_toks_sim = cosine_similarity(diff_toks_vec, matched_tok_vec)[0][0]\n",
    "        dist = nltk.edit_distance(phrase_tuple[0], x[0])\n",
    "        \n",
    "#         print(phrase_exp_sim, phrase_exp_token_sim, qry_exp_sim, dist, num_phrase_toks, num_diff_toks, num_top_match, phrase_tuple[1], phrase_tuple[0], orig_phrase)\n",
    "        out.append((qry_exp_sim, dist, num_phrase_toks, num_diff_toks, num_top_match, min_qry_phrase_sim, max_qry_phrase_sim,))\n",
    "    \n",
    "    return out\n",
    "    \n",
    "\n",
    "for topic, phrases in scores.items():\n",
    "    topic = int(topic)\n",
    "    qry_toks = [x for x in topics[topic]['topic'].lower().replace(',', '').replace('?', '').replace('\\'', '').replace('/', '').replace('’', '').split() if x not in stop]\n",
    "    qry_vec = embed(qry_toks, embeddings).reshape(1, -1)\n",
    "    for phrase_tuple in phrases:\n",
    "        phrase_vec = phrase_embeddings[phrase_tuple[0]].reshape(1, -1)\n",
    "        \n",
    "        phrase_toks = set(phrase_tuple[0].split('_'))\n",
    "        \n",
    "        phrase_tok_vec = embed(phrase_toks, embeddings).reshape(1, -1)\n",
    "        matched_vec = None \n",
    "        matched_tok_vec = None \n",
    "        diff_toks_vec = None \n",
    "        num_diff_toks = 0\n",
    "        num_phrase_toks = len(phrase_toks)\n",
    "        orig_phrase = None \n",
    "        \n",
    "        min_qry_phrase_sim = None\n",
    "        max_qry_phrase_sim = None \n",
    "        \n",
    "        num_top_match = 0 \n",
    "        \n",
    "        for x in qry_phrases[topic]:\n",
    "            if phrase_tuple[0] in x[2]:\n",
    "                num_top_match += 1 \n",
    "                    \n",
    "                # the phrase is a potential expansion of this original query phrase\n",
    "\n",
    "                orig_phrase = x[0]\n",
    "                matched_vec = phrase_embeddings[x[0]].reshape(1, -1)\n",
    "                matched_toks = set(x[0].split('_'))\n",
    "                matched_tok_vec = embed(matched_toks, embeddings).reshape(1, -1)\n",
    "                \n",
    "                if min_qry_phrase_sim is None: \n",
    "                    min_qry_phrase_sim = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                else:\n",
    "                    s = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                    if s > min_qry_phrase_sim:\n",
    "                        min_qry_phrase_sim = s \n",
    "                        \n",
    "                if max_qry_phrase_sim is None: \n",
    "                    max_qry_phrase_sim = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                else:\n",
    "                    s = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "                    if s > max_qry_phrase_sim:\n",
    "                        max_qry_phrase_sim = s \n",
    "\n",
    "                diff_toks = matched_toks - phrase_toks\n",
    "                num_diff_toks = len(diff_toks)\n",
    "                diff_toks_vec = embed(diff_toks, embeddings).reshape(1, -1)\n",
    "             \n",
    "        num_top_match = float(num_top_match) / float(len(qry_phrases[topic]))\n",
    "                \n",
    "        phrase_exp_sim = cosine_similarity(phrase_vec, matched_vec)[0][0]\n",
    "        phrase_exp_token_sim = cosine_similarity(phrase_tok_vec, matched_tok_vec)[0][0]\n",
    "        qry_exp_sim = cosine_similarity(qry_vec, phrase_tok_vec)[0][0]\n",
    "        diff_toks_sim = cosine_similarity(diff_toks_vec, matched_tok_vec)[0][0]\n",
    "        dist = nltk.edit_distance(phrase_tuple[0], x[0])\n",
    "        \n",
    "#         print(phrase_exp_sim, phrase_exp_token_sim, qry_exp_sim, dist, num_phrase_toks, num_diff_toks, num_top_match, phrase_tuple[1], phrase_tuple[0], orig_phrase)\n",
    "        X.append((qry_exp_sim, dist, num_phrase_toks, num_diff_toks, num_top_match, min_qry_phrase_sim, max_qry_phrase_sim,))\n",
    "        y.append(phrase_tuple[1])\n",
    "            \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, n_jobs=8)\n",
    "# clf = make_pipeline(StandardScaler(), SVC())\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6167168674698795"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_features = features('1', scores['1'], qry_phrases)\n",
    "predicted = clf.predict(topic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7650007, 16, 2, 1, 0.5, 0.8511563, 0.8511563), (0.698878, 18, 2, 2, 0.5, 0.83666945, 0.83666945), (0.66542274, 16, 2, 1, 0.5, 0.82565606, 0.82565606), (0.7586659, 18, 2, 1, 0.5, 0.8432792, 0.8432792), (0.7763271, 10, 3, 0, 0.5, 0.94302297, 0.94302297), (0.78233033, 10, 3, 0, 0.5, 0.95130545, 0.95130545), (0.7615414, 16, 2, 1, 0.5, 0.8150523, 0.8150523), (0.7853027, 17, 2, 1, 0.5, 0.81310725, 0.81310725), (0.7103939, 6, 2, 1, 0.5, 0.8461931, 0.8461931), (0.7971319, 16, 2, 1, 0.5, 0.8230698, 0.8230698), (0.7248093, 15, 2, 1, 0.5, 0.8413123, 0.8413123), (0.80394316, 16, 2, 0, 0.5, 0.87822735, 0.87822735), (0.8068176, 15, 2, 1, 0.5, 0.83948386, 0.83948386), (0.710752, 15, 2, 1, 0.5, 0.83719003, 0.83719003), (0.7657465, 3, 2, 1, 0.5, 0.94904065, 0.94904065), (0.81424856, 18, 2, 0, 0.5, 1.0, 1.0), (0.7367139, 17, 2, 1, 0.5, 0.86503106, 0.86503106), (0.7697695, 17, 2, 1, 0.5, 0.8303156, 0.8303156), (0.7674389, 16, 2, 1, 0.5, 0.83646023, 0.83646023), (0.7721431, 17, 2, 1, 0.5, 0.8238973, 0.8238973), (0.84377104, 4, 3, 0, 0.5, 0.89712536, 0.89712536), (0.80285233, 1, 2, 1, 0.5, 0.9304306, 0.9304306), (0.7345257, 15, 2, 1, 0.5, 0.8306037, 0.8306037), (0.79617596, 16, 2, 1, 0.5, 0.83283514, 0.83283514), (0.79976046, 16, 2, 1, 0.5, 0.86705124, 0.86705124), (0.773281, 2, 2, 1, 0.5, 0.93478996, 0.93478996), (0.77558655, 8, 3, 1, 0.5, 0.9289369, 0.9289369), (0.76772887, 3, 2, 1, 0.5, 0.92068434, 0.92068434), (0.7367139, 10, 2, 1, 0.5, 0.8394888, 0.8394888), (0.7768868, 16, 2, 1, 0.5, 0.85205954, 0.85205954), (0.773281, 16, 2, 1, 0.5, 0.90599906, 0.90599906), (0.6148736, 17, 2, 1, 0.5, 0.8345215, 0.8345215), (0.80394316, 0, 2, 0, 0.5, 0.99999994, 0.99999994), (0.8034744, 5, 3, 0, 0.5, 0.9680804, 0.9680804), (0.81877816, 15, 2, 1, 0.5, 0.8202145, 0.8202145), (0.8125763, 16, 2, 1, 0.5, 0.8459116, 0.8459116), (0.7606451, 16, 2, 1, 0.5, 0.81857455, 0.81857455), (0.7989232, 16, 2, 1, 0.5, 0.8319579, 0.8319579), (0.7406298, 16, 2, 1, 0.5, 0.8262902, 0.8262902), (0.8125457, 16, 2, 1, 0.5, 0.81631356, 0.81631356)]\n"
     ]
    }
   ],
   "source": [
    "print(topic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bring_company', -1), ('liquidators_companies', -1), ('brewing_company', -1), ('reinstate_company', 1), ('insolvent_company_liquidation', 1), ('company_voluntary_liquidation', 1), ('putting_company', -1), ('bringing_company', -1), ('voluntary_liquidation', -1), ('taking_company', -1), ('keeping_company', -1), ('liquidation_company', -1), ('liquidate_company', -1), ('solvent_company', -1), ('companies_liquidation', -1), ('reinstating_company', -1), ('insolvent_company', -1), ('reducing_company', -1), ('giving_company', -1), ('allowing_company', -1), ('company_liquidation_pay', -1), ('company_liquidations', -1), ('choosing_company', -1), ('leaving_company', -1), ('liquidating_company', -1), ('company_liquidator', -1), ('companies_into_liquidation', -1), ('company_liquidators', -1), ('company_insolvent', -1), ('suing_company', -1), ('liquidator_company', -1), ('abetting_company', -1), ('company_liquidation', -1), ('company_into_liquidation', -1), ('paying_company', -1), ('liquidated_company', -1), ('running_company', -1), ('having_company', -1), ('placing_company', -1), ('being_company', -1)]\n",
      "[-1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  1 -1 -1 -1 -1 -1  0 -1 -1\n",
      " -1  0 -1  0 -1 -1 -1  1  1  1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(scores['1'])\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bring_company', -1) -1\n",
      "('liquidators_companies', -1) 1\n",
      "('brewing_company', -1) -1\n",
      "('reinstate_company', 1) -1\n",
      "('insolvent_company_liquidation', 1) -1\n",
      "('company_voluntary_liquidation', 1) -1\n",
      "('putting_company', -1) -1\n",
      "('bringing_company', -1) -1\n",
      "('voluntary_liquidation', -1) -1\n",
      "('taking_company', -1) -1\n",
      "('keeping_company', -1) -1\n",
      "('liquidation_company', -1) -1\n",
      "('liquidate_company', -1) -1\n",
      "('solvent_company', -1) -1\n",
      "('companies_liquidation', -1) 0\n",
      "('reinstating_company', -1) 1\n",
      "('insolvent_company', -1) -1\n",
      "('reducing_company', -1) -1\n",
      "('giving_company', -1) -1\n",
      "('allowing_company', -1) -1\n",
      "('company_liquidation_pay', -1) -1\n",
      "('company_liquidations', -1) 0\n",
      "('choosing_company', -1) -1\n",
      "('leaving_company', -1) -1\n",
      "('liquidating_company', -1) -1\n",
      "('company_liquidator', -1) 0\n",
      "('companies_into_liquidation', -1) -1\n",
      "('company_liquidators', -1) 0\n",
      "('company_insolvent', -1) -1\n",
      "('suing_company', -1) -1\n",
      "('liquidator_company', -1) -1\n",
      "('abetting_company', -1) 1\n",
      "('company_liquidation', -1) 1\n",
      "('company_into_liquidation', -1) 1\n",
      "('paying_company', -1) -1\n",
      "('liquidated_company', -1) -1\n",
      "('running_company', -1) -1\n",
      "('having_company', -1) -1\n",
      "('placing_company', -1) -1\n",
      "('being_company', -1) -1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predicted)):\n",
    "    print(scores['1'][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
